{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "\n",
    "raw_data_dir = 'datasets\\sales_train_evaluation.csv'\n",
    "processed_data_dir = 'datasets\\\\'\n",
    "ORIGINAL = raw_data_dir\n",
    "BASE     = processed_data_dir+'grid_part_1.pkl'\n",
    "PRICE    = processed_data_dir+'grid_part_2.pkl'\n",
    "CALENDAR = processed_data_dir+'grid_part_3.pkl'\n",
    "LAGS     = processed_data_dir+'lags_df_28.pkl'\n",
    "MEAN_ENC = processed_data_dir+'mean_encoding_df.pkl'\n",
    "\n",
    "STORES_IDS = ['CA_1','CA_2','CA_3','CA_4','TX_1','TX_2','TX_3','WI_1','WI_2','WI_3']\n",
    "\n",
    "#LIMITS and const\n",
    "TARGET      = 'sales'            \n",
    "START_TRAIN = 0                  \n",
    "END_TRAIN   = 1941\n",
    "P_HORIZON   = 28\n",
    "\n",
    "mean_features   = ['enc_cat_id_mean','enc_cat_id_std',\n",
    "                   'enc_dept_id_mean','enc_dept_id_std',\n",
    "                   'enc_item_id_mean','enc_item_id_std'] \n",
    "\n",
    "remove_features = [TARGET,'id','state_id','store_id',\n",
    "                    'item_id', 'dept_id', 'cat_id','date','wm_yr_wk','d', \\\n",
    "                    'release', \\\n",
    "                    'price_norm', 'price_nunique', 'item_nunique', \\\n",
    "                    'price_momentum', \\\n",
    "                    'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', \\\n",
    "                    'enc_cat_id_std','event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', \\\n",
    "                    'enc_dept_id_std', 'enc_item_id_std', \\\n",
    "                    'rolling_std_7', 'rolling_std_14', \\\n",
    "                    'rolling_std_30', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', \\\n",
    "                    'rolling_mean_tmp_1_30', 'rolling_mean_tmp_7_7', \\\n",
    "                    'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30', \\\n",
    "                    'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', \\\n",
    "                    'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', \\\n",
    "                    'rolling_mean_7', 'rolling_mean_14', 'rolling_mean_30', 'rolling_mean_60', 'rolling_std_60', 'rolling_mean_180', 'rolling_std_180', 'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_60', \\\n",
    "                    'rolling_mean_tmp_14_30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "def get_data_by_store(store):\n",
    "    \n",
    "    # Read and contact basic feature\n",
    "    df = pd.concat([pd.read_pickle(BASE),\n",
    "                    pd.read_pickle(PRICE).iloc[:,2:],\n",
    "                    pd.read_pickle(CALENDAR).iloc[:,2:]],\n",
    "                    axis=1)\n",
    "    \n",
    "\n",
    "    df = df[df['d']>=START_TRAIN]\n",
    "    \n",
    "    df = df[df['store_id']==store]\n",
    "\n",
    "    df2 = pd.read_pickle(MEAN_ENC)[mean_features]\n",
    "    df2 = df2[df2.index.isin(df.index)]\n",
    "    \n",
    "    df3 = pd.read_pickle(LAGS).iloc[:,3:]\n",
    "    df3 = df3[df3.index.isin(df.index)]\n",
    "    \n",
    "    df = pd.concat([df, df2], axis=1)\n",
    "    del df2\n",
    "    \n",
    "    df = pd.concat([df, df3], axis=1)\n",
    "    del df3\n",
    "\n",
    "    state = \"snap_\" + store.split('_')[0]\n",
    "    states = ['snap_CA','snap_TX','snap_WI']\n",
    "    deleted_states = []\n",
    "    for i in states:\n",
    "        if i != state:\n",
    "            deleted_states.append(i)\n",
    "            \n",
    "    features = [col for col in list(df) if (col not in remove_features and col not in deleted_states)]\n",
    "    \n",
    "    df = df[['id','d',TARGET]+features]\n",
    "    \n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df, features\n",
    "\n",
    "# Recombine Test set after training\n",
    "def get_base_test():\n",
    "    base_test = pd.DataFrame()\n",
    "\n",
    "    for store_id in STORES_IDS:\n",
    "        temp_df = pd.read_pickle(processed_data_dir+'test_'+store_id+'.pkl')\n",
    "        temp_df['store_id'] = store_id\n",
    "        base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)\n",
    "    \n",
    "    return base_test\n",
    "\n",
    "# split(trainning and validation)\n",
    "def split(dataframe_for_splitting):\n",
    "    train = dataframe_for_splitting[dataframe_for_splitting[\"d\"] <= END_TRAIN - P_HORIZON]\n",
    "    validation = dataframe_for_splitting[(dataframe_for_splitting[\"d\"] <= END_TRAIN) & (dataframe_for_splitting[\"d\"] > END_TRAIN - P_HORIZON)]\n",
    "    return train, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1          19.6576           25.78m\n",
      "         2          19.4772           25.47m\n",
      "         3          19.3003           25.17m\n",
      "         4          19.1269           25.12m\n",
      "         5          18.9524           25.19m\n",
      "         6          18.7853           25.06m\n",
      "         7          18.6173           24.84m\n",
      "         8          18.4565           24.67m\n",
      "         9          18.2986           24.44m\n",
      "        10          18.1389           24.26m\n",
      "        20          16.7097           30.32m\n",
      "        30          15.5040           31.31m\n",
      "        40          14.4904           30.27m\n",
      "        50          13.6251           28.10m\n",
      "        60          12.8862           25.88m\n",
      "        70          12.2547           23.38m\n",
      "        80          11.7187           20.70m\n",
      "        90          11.2605           17.90m\n",
      "       100          10.8685           15.02m\n",
      "Model for store CA_1 saved as model_store_CA_1_GB_0218.joblib\n",
      "Iteration 1: Training Loss = 19.657612959164908, Validation Loss = 12.819905255035216\n",
      "Iteration 2: Training Loss = 19.47722018241013, Validation Loss = 12.69954481394863\n",
      "Iteration 3: Training Loss = 19.30030669439414, Validation Loss = 12.581189136766737\n",
      "Iteration 4: Training Loss = 19.126900619965205, Validation Loss = 12.459472982314788\n",
      "Iteration 5: Training Loss = 18.952381691495468, Validation Loss = 12.340903852846392\n",
      "Iteration 6: Training Loss = 18.785329681717247, Validation Loss = 12.229347472811947\n",
      "Iteration 7: Training Loss = 18.61729949686522, Validation Loss = 12.115326019455392\n",
      "Iteration 8: Training Loss = 18.45651114748345, Validation Loss = 12.009342346426504\n",
      "Iteration 9: Training Loss = 18.298608700670236, Validation Loss = 11.903949773863781\n",
      "Iteration 10: Training Loss = 18.13887216247879, Validation Loss = 11.795978429985052\n",
      "Iteration 11: Training Loss = 17.984974821394847, Validation Loss = 11.687499955855701\n",
      "Iteration 12: Training Loss = 17.83179783295274, Validation Loss = 11.583889554105639\n",
      "Iteration 13: Training Loss = 17.682386416198256, Validation Loss = 11.484617497910476\n",
      "Iteration 14: Training Loss = 17.53614119585379, Validation Loss = 11.386579504845825\n",
      "Iteration 15: Training Loss = 17.39450892570196, Validation Loss = 11.291962834362733\n",
      "Iteration 16: Training Loss = 17.25046810818363, Validation Loss = 11.195433872928088\n",
      "Iteration 17: Training Loss = 17.1105034158466, Validation Loss = 11.098115888210518\n",
      "Iteration 18: Training Loss = 16.97632638667912, Validation Loss = 11.006569695445227\n",
      "Iteration 19: Training Loss = 16.84136962117089, Validation Loss = 10.912313670917682\n",
      "Iteration 20: Training Loss = 16.709658217964904, Validation Loss = 10.82426509514752\n",
      "Iteration 21: Training Loss = 16.579618034335258, Validation Loss = 10.73473883663892\n",
      "Iteration 22: Training Loss = 16.45285428720091, Validation Loss = 10.64927500333879\n",
      "Iteration 23: Training Loss = 16.32518159123198, Validation Loss = 10.563991490044916\n",
      "Iteration 24: Training Loss = 16.20182464262792, Validation Loss = 10.478890008086896\n",
      "Iteration 25: Training Loss = 16.081566774524518, Validation Loss = 10.39830525925098\n",
      "Iteration 26: Training Loss = 15.959965435126593, Validation Loss = 10.316637251532917\n",
      "Iteration 27: Training Loss = 15.843313312621804, Validation Loss = 10.241536440151764\n",
      "Iteration 28: Training Loss = 15.729113076247073, Validation Loss = 10.165600923256179\n",
      "Iteration 29: Training Loss = 15.615052977824027, Validation Loss = 10.088628379012599\n",
      "Iteration 30: Training Loss = 15.503997548587487, Validation Loss = 10.010344784437965\n",
      "Iteration 31: Training Loss = 15.392489170372535, Validation Loss = 9.937642385865328\n",
      "Iteration 32: Training Loss = 15.284846167600792, Validation Loss = 9.869407280775595\n",
      "Iteration 33: Training Loss = 15.180354586812673, Validation Loss = 9.80031946244648\n",
      "Iteration 34: Training Loss = 15.07827647133782, Validation Loss = 9.727642321344433\n",
      "Iteration 35: Training Loss = 14.976500170210043, Validation Loss = 9.655784744975941\n",
      "Iteration 36: Training Loss = 14.877139982753665, Validation Loss = 9.58895767738846\n",
      "Iteration 37: Training Loss = 14.776018787552003, Validation Loss = 9.52180121377452\n",
      "Iteration 38: Training Loss = 14.678770166037403, Validation Loss = 9.460493839744908\n",
      "Iteration 39: Training Loss = 14.58377419046341, Validation Loss = 9.400708022206233\n",
      "Iteration 40: Training Loss = 14.490372556230733, Validation Loss = 9.334860237384376\n",
      "Iteration 41: Training Loss = 14.396505471902609, Validation Loss = 9.274611236266011\n",
      "Iteration 42: Training Loss = 14.305991644709298, Validation Loss = 9.218455290335134\n",
      "Iteration 43: Training Loss = 14.215376236808934, Validation Loss = 9.156511699506513\n",
      "Iteration 44: Training Loss = 14.128100456401562, Validation Loss = 9.094733498000439\n",
      "Iteration 45: Training Loss = 14.03872373401364, Validation Loss = 9.03785903113829\n",
      "Iteration 46: Training Loss = 13.954073410281339, Validation Loss = 8.984493435463493\n",
      "Iteration 47: Training Loss = 13.871670700151972, Validation Loss = 8.929404466398234\n",
      "Iteration 48: Training Loss = 13.788159870251409, Validation Loss = 8.875927488797881\n",
      "Iteration 49: Training Loss = 13.70477331709385, Validation Loss = 8.822813569357818\n",
      "Iteration 50: Training Loss = 13.625132426938409, Validation Loss = 8.774208405655122\n",
      "Iteration 51: Training Loss = 13.547567118522801, Validation Loss = 8.71886512282822\n",
      "Iteration 52: Training Loss = 13.469230729942648, Validation Loss = 8.666325390785452\n",
      "Iteration 53: Training Loss = 13.391192166424961, Validation Loss = 8.617260230549867\n",
      "Iteration 54: Training Loss = 13.31665982574223, Validation Loss = 8.57196012097237\n",
      "Iteration 55: Training Loss = 13.24267169377223, Validation Loss = 8.522566198467214\n",
      "Iteration 56: Training Loss = 13.168284274682119, Validation Loss = 8.475546347872443\n",
      "Iteration 57: Training Loss = 13.09742005480323, Validation Loss = 8.430301223449653\n",
      "Iteration 58: Training Loss = 13.025978916660653, Validation Loss = 8.384655931675217\n",
      "Iteration 59: Training Loss = 12.957514136729166, Validation Loss = 8.337120858022187\n",
      "Iteration 60: Training Loss = 12.886237494365412, Validation Loss = 8.291698685446304\n",
      "Iteration 61: Training Loss = 12.820426049046116, Validation Loss = 8.245070537674803\n",
      "Iteration 62: Training Loss = 12.752410090573052, Validation Loss = 8.202296566353448\n",
      "Iteration 63: Training Loss = 12.686948179553921, Validation Loss = 8.158410527107849\n",
      "Iteration 64: Training Loss = 12.623355506852297, Validation Loss = 8.118099723881322\n",
      "Iteration 65: Training Loss = 12.561468042712601, Validation Loss = 8.077775934775277\n",
      "Iteration 66: Training Loss = 12.496591040107925, Validation Loss = 8.036760243319593\n",
      "Iteration 67: Training Loss = 12.433059219521505, Validation Loss = 7.996431365726572\n",
      "Iteration 68: Training Loss = 12.372595132412561, Validation Loss = 7.956314093539806\n",
      "Iteration 69: Training Loss = 12.314571285297307, Validation Loss = 7.917412328817749\n",
      "Iteration 70: Training Loss = 12.254658094048827, Validation Loss = 7.88056168191983\n",
      "Iteration 71: Training Loss = 12.197827545817185, Validation Loss = 7.843969688473488\n",
      "Iteration 72: Training Loss = 12.140529613316174, Validation Loss = 7.806096589547278\n",
      "Iteration 73: Training Loss = 12.086129411078348, Validation Loss = 7.771165088391389\n",
      "Iteration 74: Training Loss = 12.029344098723094, Validation Loss = 7.73475665201849\n",
      "Iteration 75: Training Loss = 11.976645930167146, Validation Loss = 7.702241112627441\n",
      "Iteration 76: Training Loss = 11.922247955135083, Validation Loss = 7.6688900142123675\n",
      "Iteration 77: Training Loss = 11.870057087420381, Validation Loss = 7.634401118073865\n",
      "Iteration 78: Training Loss = 11.819764149067163, Validation Loss = 7.6036196082064045\n",
      "Iteration 79: Training Loss = 11.770400728682667, Validation Loss = 7.572114059317799\n",
      "Iteration 80: Training Loss = 11.718700151253426, Validation Loss = 7.539044448626503\n",
      "Iteration 81: Training Loss = 11.670829147268174, Validation Loss = 7.508561198648949\n",
      "Iteration 82: Training Loss = 11.620740172186997, Validation Loss = 7.476587618944586\n",
      "Iteration 83: Training Loss = 11.573376299246412, Validation Loss = 7.445359470722514\n",
      "Iteration 84: Training Loss = 11.527441196602727, Validation Loss = 7.416299741061747\n",
      "Iteration 85: Training Loss = 11.48029554665426, Validation Loss = 7.387667776239722\n",
      "Iteration 86: Training Loss = 11.435688695298257, Validation Loss = 7.359304073051246\n",
      "Iteration 87: Training Loss = 11.39172223419458, Validation Loss = 7.33181272866885\n",
      "Iteration 88: Training Loss = 11.346197474028122, Validation Loss = 7.302193828713258\n",
      "Iteration 89: Training Loss = 11.304196019353327, Validation Loss = 7.27528655629155\n",
      "Iteration 90: Training Loss = 11.260545788399586, Validation Loss = 7.248780669606761\n",
      "Iteration 91: Training Loss = 11.218691970936563, Validation Loss = 7.223067097140112\n",
      "Iteration 92: Training Loss = 11.178500099413204, Validation Loss = 7.197721443392366\n",
      "Iteration 93: Training Loss = 11.136269230448274, Validation Loss = 7.171034674094638\n",
      "Iteration 94: Training Loss = 11.096842380912287, Validation Loss = 7.146453125411388\n",
      "Iteration 95: Training Loss = 11.05643520090395, Validation Loss = 7.121775270408382\n",
      "Iteration 96: Training Loss = 11.016229548544983, Validation Loss = 7.095515479323774\n",
      "Iteration 97: Training Loss = 10.978997664035656, Validation Loss = 7.07375433953886\n",
      "Iteration 98: Training Loss = 10.942277383419494, Validation Loss = 7.050420657535535\n",
      "Iteration 99: Training Loss = 10.905873000189342, Validation Loss = 7.02820091267058\n",
      "Iteration 100: Training Loss = 10.868482370929105, Validation Loss = 7.005306957424572\n",
      "Iteration 101: Training Loss = 10.831988330314402, Validation Loss = 6.983282562213865\n",
      "Iteration 102: Training Loss = 10.797798268925096, Validation Loss = 6.961738509863289\n",
      "Iteration 103: Training Loss = 10.761588717093217, Validation Loss = 6.938817194915867\n",
      "Iteration 104: Training Loss = 10.728544392086466, Validation Loss = 6.916824413148986\n",
      "Iteration 105: Training Loss = 10.693583582185262, Validation Loss = 6.8941383209043865\n",
      "Iteration 106: Training Loss = 10.661065530330943, Validation Loss = 6.875228102595125\n",
      "Iteration 107: Training Loss = 10.627350147392125, Validation Loss = 6.855076245570095\n",
      "Iteration 108: Training Loss = 10.59616062860133, Validation Loss = 6.8350930994764765\n",
      "Iteration 109: Training Loss = 10.56329104795301, Validation Loss = 6.8139686602782845\n",
      "Iteration 110: Training Loss = 10.530926535946012, Validation Loss = 6.7941283587264145\n",
      "Iteration 111: Training Loss = 10.500960493560406, Validation Loss = 6.7745591797414555\n",
      "Iteration 112: Training Loss = 10.469521430721516, Validation Loss = 6.755154264411788\n",
      "Iteration 113: Training Loss = 10.440220848265358, Validation Loss = 6.7366850426183795\n",
      "Iteration 114: Training Loss = 10.409538811366751, Validation Loss = 6.718742968970879\n",
      "Iteration 115: Training Loss = 10.381367318020342, Validation Loss = 6.70232501039359\n",
      "Iteration 116: Training Loss = 10.351831922874652, Validation Loss = 6.683438740842097\n",
      "Iteration 117: Training Loss = 10.324642408898807, Validation Loss = 6.667100536537661\n",
      "Iteration 118: Training Loss = 10.295777630215468, Validation Loss = 6.65009648255764\n",
      "Iteration 119: Training Loss = 10.269336234350488, Validation Loss = 6.6330873392560115\n",
      "Iteration 120: Training Loss = 10.243001802224141, Validation Loss = 6.618919849367812\n",
      "Iteration 121: Training Loss = 10.215635527965414, Validation Loss = 6.601820165143377\n",
      "Iteration 122: Training Loss = 10.190073750626615, Validation Loss = 6.585926430177135\n",
      "Iteration 123: Training Loss = 10.163361044956433, Validation Loss = 6.570235293106748\n",
      "Iteration 124: Training Loss = 10.138919565681803, Validation Loss = 6.555644023656132\n",
      "Iteration 125: Training Loss = 10.113127551885698, Validation Loss = 6.539858417701605\n",
      "Iteration 126: Training Loss = 10.089146506704147, Validation Loss = 6.524407157766291\n",
      "Iteration 127: Training Loss = 10.065767356548424, Validation Loss = 6.509874485889376\n",
      "Iteration 128: Training Loss = 10.040960620958376, Validation Loss = 6.49536064014274\n",
      "Iteration 129: Training Loss = 10.016671788608805, Validation Loss = 6.479736685927708\n",
      "Iteration 130: Training Loss = 9.994160482636508, Validation Loss = 6.465804293878347\n",
      "Iteration 131: Training Loss = 9.970519647129034, Validation Loss = 6.452173975449493\n",
      "Iteration 132: Training Loss = 9.947355494171786, Validation Loss = 6.4379342606700725\n",
      "Iteration 133: Training Loss = 9.926074604144382, Validation Loss = 6.424789199622583\n",
      "Iteration 134: Training Loss = 9.905058566255699, Validation Loss = 6.41190638591613\n",
      "Iteration 135: Training Loss = 9.882821711660704, Validation Loss = 6.398503515249761\n",
      "Iteration 136: Training Loss = 9.86077488110951, Validation Loss = 6.385050486825326\n",
      "Iteration 137: Training Loss = 9.840358443261572, Validation Loss = 6.374239288049288\n",
      "Iteration 138: Training Loss = 9.819229985660229, Validation Loss = 6.360578483856022\n",
      "Iteration 139: Training Loss = 9.799780023245466, Validation Loss = 6.347995977696896\n",
      "Iteration 140: Training Loss = 9.778849584723808, Validation Loss = 6.336310202534559\n",
      "Iteration 141: Training Loss = 9.759960398910213, Validation Loss = 6.3247884224602595\n",
      "Iteration 142: Training Loss = 9.73973168123307, Validation Loss = 6.313258317884239\n",
      "Iteration 143: Training Loss = 9.721302147301959, Validation Loss = 6.301892498213632\n",
      "Iteration 144: Training Loss = 9.701932176918316, Validation Loss = 6.289743978297372\n",
      "Iteration 145: Training Loss = 9.684055712540681, Validation Loss = 6.278828916428648\n",
      "Iteration 146: Training Loss = 9.66508828139204, Validation Loss = 6.268148019851032\n",
      "Iteration 147: Training Loss = 9.64628504242123, Validation Loss = 6.25763445105127\n",
      "Iteration 148: Training Loss = 9.62922444545686, Validation Loss = 6.24750208527076\n",
      "Iteration 149: Training Loss = 9.612421685222598, Validation Loss = 6.236605883347641\n",
      "Iteration 150: Training Loss = 9.595974333681031, Validation Loss = 6.22637447051958\n",
      "Mean Squared Error on Validation Set: 6.22637447051958\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           9.8007           42.24m\n",
      "         2           9.7279           42.04m\n",
      "         3           9.6565           41.43m\n",
      "         4           9.5858           41.13m\n",
      "         5           9.5171           40.81m\n",
      "         6           9.4492           40.66m\n",
      "         7           9.3830           40.33m\n",
      "         8           9.3177           40.06m\n",
      "         9           9.2541           39.68m\n",
      "        10           9.1911           39.33m\n",
      "        20           8.6184           36.36m\n",
      "        30           8.1338           33.47m\n",
      "        40           7.7236           30.75m\n",
      "        50           7.3757           28.10m\n",
      "        60           7.0795           25.32m\n",
      "        70           6.8240           22.53m\n",
      "        80           6.6055           19.73m\n",
      "        90           6.4179           16.91m\n",
      "       100           6.2568           14.09m\n",
      "Model for store CA_2 saved as model_store_CA_2_GB_0218.joblib\n",
      "Iteration 1: Training Loss = 9.800746139084321, Validation Loss = 9.919916573917785\n",
      "Iteration 2: Training Loss = 9.727874657253516, Validation Loss = 9.840384236611783\n",
      "Iteration 3: Training Loss = 9.656450965434871, Validation Loss = 9.762245340235372\n",
      "Iteration 4: Training Loss = 9.585849278985268, Validation Loss = 9.686757673782989\n",
      "Iteration 5: Training Loss = 9.517126799495978, Validation Loss = 9.611372622106861\n",
      "Iteration 6: Training Loss = 9.4491879244508, Validation Loss = 9.538592255935695\n",
      "Iteration 7: Training Loss = 9.383047470372773, Validation Loss = 9.465853047723268\n",
      "Iteration 8: Training Loss = 9.317676542265147, Validation Loss = 9.395686712665439\n",
      "Iteration 9: Training Loss = 9.254068294657863, Validation Loss = 9.325293676983577\n",
      "Iteration 10: Training Loss = 9.19105444494229, Validation Loss = 9.259199043566325\n",
      "Iteration 11: Training Loss = 9.1292939801092, Validation Loss = 9.194142410031743\n",
      "Iteration 12: Training Loss = 9.068953435465415, Validation Loss = 9.128406679574692\n",
      "Iteration 13: Training Loss = 9.009415243761996, Validation Loss = 9.065747956855834\n",
      "Iteration 14: Training Loss = 8.950659837054994, Validation Loss = 9.001430246789928\n",
      "Iteration 15: Training Loss = 8.892447552109294, Validation Loss = 8.939980405934172\n",
      "Iteration 16: Training Loss = 8.836236822345318, Validation Loss = 8.878666835188675\n",
      "Iteration 17: Training Loss = 8.781218324309378, Validation Loss = 8.817502700359682\n",
      "Iteration 18: Training Loss = 8.726001666009113, Validation Loss = 8.759101322469334\n",
      "Iteration 19: Training Loss = 8.671664267363356, Validation Loss = 8.699205890678597\n",
      "Iteration 20: Training Loss = 8.618397196536918, Validation Loss = 8.642348896878586\n",
      "Iteration 21: Training Loss = 8.56629335584626, Validation Loss = 8.586039614895396\n",
      "Iteration 22: Training Loss = 8.51484787275839, Validation Loss = 8.52948777159331\n",
      "Iteration 23: Training Loss = 8.463915856018266, Validation Loss = 8.473099193787633\n",
      "Iteration 24: Training Loss = 8.414343502029372, Validation Loss = 8.417794192195409\n",
      "Iteration 25: Training Loss = 8.365653525839388, Validation Loss = 8.364051938173759\n",
      "Iteration 26: Training Loss = 8.31745216247736, Validation Loss = 8.310512086211016\n",
      "Iteration 27: Training Loss = 8.270479680034965, Validation Loss = 8.258268557011963\n",
      "Iteration 28: Training Loss = 8.223990114215548, Validation Loss = 8.20851010293424\n",
      "Iteration 29: Training Loss = 8.178901767546888, Validation Loss = 8.158656603330376\n",
      "Iteration 30: Training Loss = 8.133761949380485, Validation Loss = 8.108975492431972\n",
      "Iteration 31: Training Loss = 8.089540225128369, Validation Loss = 8.059348944433747\n",
      "Iteration 32: Training Loss = 8.046091897326304, Validation Loss = 8.012816176883959\n",
      "Iteration 33: Training Loss = 8.003316622536309, Validation Loss = 7.965305932710634\n",
      "Iteration 34: Training Loss = 7.961286341939154, Validation Loss = 7.917419680304292\n",
      "Iteration 35: Training Loss = 7.919950826158947, Validation Loss = 7.873221580248662\n",
      "Iteration 36: Training Loss = 7.8793584073382155, Validation Loss = 7.828445897105043\n",
      "Iteration 37: Training Loss = 7.839270418698123, Validation Loss = 7.783868104403972\n",
      "Iteration 38: Training Loss = 7.8000544048404485, Validation Loss = 7.740053777866401\n",
      "Iteration 39: Training Loss = 7.761516379002766, Validation Loss = 7.696901499810731\n",
      "Iteration 40: Training Loss = 7.723616240765927, Validation Loss = 7.6552499039291435\n",
      "Iteration 41: Training Loss = 7.686181271396571, Validation Loss = 7.613251287609655\n",
      "Iteration 42: Training Loss = 7.649299240980376, Validation Loss = 7.572275614811194\n",
      "Iteration 43: Training Loss = 7.613222754044855, Validation Loss = 7.53136668263126\n",
      "Iteration 44: Training Loss = 7.577580799892366, Validation Loss = 7.49049989398033\n",
      "Iteration 45: Training Loss = 7.542521251372184, Validation Loss = 7.451695341797069\n",
      "Iteration 46: Training Loss = 7.508272086108493, Validation Loss = 7.413027482752535\n",
      "Iteration 47: Training Loss = 7.47403973002442, Validation Loss = 7.376372182110026\n",
      "Iteration 48: Training Loss = 7.440742865048996, Validation Loss = 7.33739442643078\n",
      "Iteration 49: Training Loss = 7.407961670466331, Validation Loss = 7.300746963427174\n",
      "Iteration 50: Training Loss = 7.375692730715815, Validation Loss = 7.2631438138123565\n",
      "Iteration 51: Training Loss = 7.344235243489126, Validation Loss = 7.227485843491992\n",
      "Iteration 52: Training Loss = 7.312884266642959, Validation Loss = 7.190819413104363\n",
      "Iteration 53: Training Loss = 7.282216226882555, Validation Loss = 7.1562473321224225\n",
      "Iteration 54: Training Loss = 7.252137598868058, Validation Loss = 7.1222702214097895\n",
      "Iteration 55: Training Loss = 7.2223131763824355, Validation Loss = 7.0892271792785975\n",
      "Iteration 56: Training Loss = 7.192961165832755, Validation Loss = 7.055078918655137\n",
      "Iteration 57: Training Loss = 7.163387540385669, Validation Loss = 7.0231731441384\n",
      "Iteration 58: Training Loss = 7.135012559818002, Validation Loss = 6.989819034092502\n",
      "Iteration 59: Training Loss = 7.107011330522834, Validation Loss = 6.957501855643554\n",
      "Iteration 60: Training Loss = 7.079490513238245, Validation Loss = 6.926354614811229\n",
      "Iteration 61: Training Loss = 7.051412162382207, Validation Loss = 6.894804436691251\n",
      "Iteration 62: Training Loss = 7.0247514421789825, Validation Loss = 6.863789706692822\n",
      "Iteration 63: Training Loss = 6.99852953655646, Validation Loss = 6.833023082606404\n",
      "Iteration 64: Training Loss = 6.972732691720552, Validation Loss = 6.803767742284511\n",
      "Iteration 65: Training Loss = 6.9473330563363, Validation Loss = 6.774810105901015\n",
      "Iteration 66: Training Loss = 6.921666042855921, Validation Loss = 6.747019825567791\n",
      "Iteration 67: Training Loss = 6.8970805687449, Validation Loss = 6.71765927432273\n",
      "Iteration 68: Training Loss = 6.871946687214141, Validation Loss = 6.6891662419738696\n",
      "Iteration 69: Training Loss = 6.848117692706997, Validation Loss = 6.662219912875106\n",
      "Iteration 70: Training Loss = 6.824011954936975, Validation Loss = 6.635622497621123\n",
      "Iteration 71: Training Loss = 6.800723298214437, Validation Loss = 6.60855093342664\n",
      "Iteration 72: Training Loss = 6.777984708610568, Validation Loss = 6.581302409080492\n",
      "Iteration 73: Training Loss = 6.754680445747565, Validation Loss = 6.554789875254985\n",
      "Iteration 74: Training Loss = 6.732672284474768, Validation Loss = 6.528451998380586\n",
      "Iteration 75: Training Loss = 6.709968828375863, Validation Loss = 6.50370008161733\n",
      "Iteration 76: Training Loss = 6.688484556424771, Validation Loss = 6.478156203919656\n",
      "Iteration 77: Training Loss = 6.666864294321229, Validation Loss = 6.454519580702098\n",
      "Iteration 78: Training Loss = 6.646088021599048, Validation Loss = 6.429883281820965\n",
      "Iteration 79: Training Loss = 6.625720535295533, Validation Loss = 6.405934889385583\n",
      "Iteration 80: Training Loss = 6.60552335223547, Validation Loss = 6.381992345744919\n",
      "Iteration 81: Training Loss = 6.584770908387776, Validation Loss = 6.3593352656131445\n",
      "Iteration 82: Training Loss = 6.565144747491595, Validation Loss = 6.336595910787304\n",
      "Iteration 83: Training Loss = 6.545994754198999, Validation Loss = 6.313810727675688\n",
      "Iteration 84: Training Loss = 6.527085888667237, Validation Loss = 6.291161804486378\n",
      "Iteration 85: Training Loss = 6.507592674887513, Validation Loss = 6.2697865075460575\n",
      "Iteration 86: Training Loss = 6.489279865411663, Validation Loss = 6.2476833473483815\n",
      "Iteration 87: Training Loss = 6.471268827892066, Validation Loss = 6.226437248991875\n",
      "Iteration 88: Training Loss = 6.452646085819104, Validation Loss = 6.2059908186437776\n",
      "Iteration 89: Training Loss = 6.435135579894836, Validation Loss = 6.184742467172103\n",
      "Iteration 90: Training Loss = 6.417901143087856, Validation Loss = 6.163994661792349\n",
      "Iteration 91: Training Loss = 6.4003430587138785, Validation Loss = 6.144701174103502\n",
      "Iteration 92: Training Loss = 6.383640737652242, Validation Loss = 6.12424007322477\n",
      "Iteration 93: Training Loss = 6.367252846220135, Validation Loss = 6.104250311058824\n",
      "Iteration 94: Training Loss = 6.350229071027577, Validation Loss = 6.0853072070218905\n",
      "Iteration 95: Training Loss = 6.3343171582556845, Validation Loss = 6.065857334213425\n",
      "Iteration 96: Training Loss = 6.317824313132389, Validation Loss = 6.04761167846234\n",
      "Iteration 97: Training Loss = 6.302393789085793, Validation Loss = 6.02903643408252\n",
      "Iteration 98: Training Loss = 6.286613230541065, Validation Loss = 6.01151499804952\n",
      "Iteration 99: Training Loss = 6.27166872499414, Validation Loss = 5.99363923838879\n",
      "Iteration 100: Training Loss = 6.256803931754634, Validation Loss = 5.975869981035394\n",
      "Iteration 101: Training Loss = 6.241470480181308, Validation Loss = 5.95864313853659\n",
      "Iteration 102: Training Loss = 6.2272243872407325, Validation Loss = 5.941436365461872\n",
      "Iteration 103: Training Loss = 6.2126112764553545, Validation Loss = 5.924955798243382\n",
      "Iteration 104: Training Loss = 6.19876234845228, Validation Loss = 5.907939971811119\n",
      "Iteration 105: Training Loss = 6.184348115335725, Validation Loss = 5.89172598172572\n",
      "Iteration 106: Training Loss = 6.170965363434658, Validation Loss = 5.8748122173695885\n",
      "Iteration 107: Training Loss = 6.157762745447963, Validation Loss = 5.858437974605405\n",
      "Iteration 108: Training Loss = 6.143997393457636, Validation Loss = 5.843008990128619\n",
      "Iteration 109: Training Loss = 6.13102057475753, Validation Loss = 5.827139742063392\n",
      "Iteration 110: Training Loss = 6.118348804383564, Validation Loss = 5.81137252732557\n",
      "Iteration 111: Training Loss = 6.105394199077096, Validation Loss = 5.796893898297392\n",
      "Iteration 112: Training Loss = 6.093034091735231, Validation Loss = 5.7819137641768\n",
      "Iteration 113: Training Loss = 6.08025455690907, Validation Loss = 5.767377737757852\n",
      "Iteration 114: Training Loss = 6.068253417465606, Validation Loss = 5.752604042626308\n",
      "Iteration 115: Training Loss = 6.056512892797345, Validation Loss = 5.738050418226867\n",
      "Iteration 116: Training Loss = 6.044292619856595, Validation Loss = 5.72418669552889\n",
      "Iteration 117: Training Loss = 6.032797602336861, Validation Loss = 5.709793886996705\n",
      "Iteration 118: Training Loss = 6.021442418554911, Validation Loss = 5.695825759103465\n",
      "Iteration 119: Training Loss = 6.010378583999424, Validation Loss = 5.68216443142407\n",
      "Iteration 120: Training Loss = 5.998859010958056, Validation Loss = 5.668905877656572\n",
      "Iteration 121: Training Loss = 5.987660248963589, Validation Loss = 5.656262161727366\n",
      "Iteration 122: Training Loss = 5.977115897485484, Validation Loss = 5.643281854549555\n",
      "Iteration 123: Training Loss = 5.966709473610309, Validation Loss = 5.6299424086200585\n",
      "Iteration 124: Training Loss = 5.955950518724412, Validation Loss = 5.616798279971854\n",
      "Iteration 125: Training Loss = 5.945751044806984, Validation Loss = 5.604106192143898\n",
      "Iteration 126: Training Loss = 5.935815465910053, Validation Loss = 5.591751436798717\n",
      "Iteration 127: Training Loss = 5.925778383792463, Validation Loss = 5.5797045206265095\n",
      "Iteration 128: Training Loss = 5.915544573814096, Validation Loss = 5.568056720073316\n",
      "Iteration 129: Training Loss = 5.90601872955095, Validation Loss = 5.556217186814243\n",
      "Iteration 130: Training Loss = 5.896542515243492, Validation Loss = 5.544094824634343\n",
      "Iteration 131: Training Loss = 5.8867269390478265, Validation Loss = 5.532700379936005\n",
      "Iteration 132: Training Loss = 5.877637557821373, Validation Loss = 5.521323861225064\n",
      "Iteration 133: Training Loss = 5.868273764491386, Validation Loss = 5.51050164119884\n",
      "Iteration 134: Training Loss = 5.8594819980282224, Validation Loss = 5.4988144376661925\n",
      "Iteration 135: Training Loss = 5.850803424679538, Validation Loss = 5.487732393084817\n",
      "Iteration 136: Training Loss = 5.841698972476973, Validation Loss = 5.477010630907283\n",
      "Iteration 137: Training Loss = 5.833061180294277, Validation Loss = 5.466491195887531\n",
      "Iteration 138: Training Loss = 5.824799564670952, Validation Loss = 5.456011461908061\n",
      "Iteration 139: Training Loss = 5.816563279540005, Validation Loss = 5.445791013822395\n",
      "Iteration 140: Training Loss = 5.80797432958506, Validation Loss = 5.435711783126827\n",
      "Iteration 141: Training Loss = 5.800016977890195, Validation Loss = 5.426009989538041\n",
      "Iteration 142: Training Loss = 5.791813223401868, Validation Loss = 5.416496169484357\n",
      "Iteration 143: Training Loss = 5.7840559659026, Validation Loss = 5.406769793286329\n",
      "Iteration 144: Training Loss = 5.7759797249037765, Validation Loss = 5.397045817539454\n",
      "Iteration 145: Training Loss = 5.768429437908149, Validation Loss = 5.387254787095911\n",
      "Iteration 146: Training Loss = 5.760872750735785, Validation Loss = 5.378059739465385\n",
      "Iteration 147: Training Loss = 5.7536266903249995, Validation Loss = 5.3692535629772244\n",
      "Iteration 148: Training Loss = 5.7460395849970505, Validation Loss = 5.359905386198648\n",
      "Iteration 149: Training Loss = 5.7389627707551805, Validation Loss = 5.351283002882777\n",
      "Iteration 150: Training Loss = 5.731910380231781, Validation Loss = 5.342454599367183\n",
      "Mean Squared Error on Validation Set: 5.342454599367183\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1          46.4102           50.84m\n",
      "         2          45.9168           50.12m\n",
      "         3          45.4332           49.75m\n",
      "         4          44.9595           49.41m\n",
      "         5          44.4946           49.34m\n",
      "         6          44.0430           48.95m\n",
      "         7          43.5956           48.62m\n",
      "         8          43.1575           48.34m\n",
      "         9          42.7327           47.94m\n",
      "        10          42.3100           47.65m\n",
      "        20          38.4986           43.92m\n",
      "        30          35.3047           40.64m\n",
      "        40          32.6094           37.17m\n",
      "        50          30.3472           33.85m\n",
      "        60          28.4252           30.49m\n",
      "        70          26.7937           27.13m\n",
      "        80          25.4179           23.76m\n",
      "        90          24.2406           19.53m\n",
      "       100          23.2460           15.69m\n",
      "Model for store CA_3 saved as model_store_CA_3_GB_0218.joblib\n",
      "Iteration 1: Training Loss = 46.41021466942331, Validation Loss = 22.964140187945706\n",
      "Iteration 2: Training Loss = 45.916784201301944, Validation Loss = 22.666666442292147\n",
      "Iteration 3: Training Loss = 45.43315833103087, Validation Loss = 22.375422649741274\n",
      "Iteration 4: Training Loss = 44.959467021449036, Validation Loss = 22.088813642672857\n",
      "Iteration 5: Training Loss = 44.49455483410032, Validation Loss = 21.81013740516094\n",
      "Iteration 6: Training Loss = 44.04299110314072, Validation Loss = 21.538184445300114\n",
      "Iteration 7: Training Loss = 43.59562496073198, Validation Loss = 21.273190862073598\n",
      "Iteration 8: Training Loss = 43.15748460167836, Validation Loss = 21.009685673520615\n",
      "Iteration 9: Training Loss = 42.73269703765632, Validation Loss = 20.725962050149665\n",
      "Iteration 10: Training Loss = 42.30998138024565, Validation Loss = 20.477948990557962\n",
      "Iteration 11: Training Loss = 41.89551361650012, Validation Loss = 20.234206491791625\n",
      "Iteration 12: Training Loss = 41.49355119771238, Validation Loss = 19.967059791928822\n",
      "Iteration 13: Training Loss = 41.094652347601915, Validation Loss = 19.699272846735862\n",
      "Iteration 14: Training Loss = 40.7047364487632, Validation Loss = 19.449532666575067\n",
      "Iteration 15: Training Loss = 40.318736233220285, Validation Loss = 19.19248381484958\n",
      "Iteration 16: Training Loss = 39.939069928655705, Validation Loss = 18.97045835435564\n",
      "Iteration 17: Training Loss = 39.56860072721311, Validation Loss = 18.724191207172755\n",
      "Iteration 18: Training Loss = 39.20591228753756, Validation Loss = 18.494907498920377\n",
      "Iteration 19: Training Loss = 38.851243554474856, Validation Loss = 18.26339052351478\n",
      "Iteration 20: Training Loss = 38.49864090943574, Validation Loss = 18.031706021818213\n",
      "Iteration 21: Training Loss = 38.155861727364865, Validation Loss = 17.8217256311583\n",
      "Iteration 22: Training Loss = 37.815834462314285, Validation Loss = 17.602805622097307\n",
      "Iteration 23: Training Loss = 37.47936451479923, Validation Loss = 17.41044265267441\n",
      "Iteration 24: Training Loss = 37.152453075882434, Validation Loss = 17.197879652084985\n",
      "Iteration 25: Training Loss = 36.827957247651, Validation Loss = 17.01165737764071\n",
      "Iteration 26: Training Loss = 36.50977041634871, Validation Loss = 16.812379238885647\n",
      "Iteration 27: Training Loss = 36.20086876675916, Validation Loss = 16.644542277103408\n",
      "Iteration 28: Training Loss = 35.89477083077832, Validation Loss = 16.451017308508312\n",
      "Iteration 29: Training Loss = 35.59793814873034, Validation Loss = 16.292065042607504\n",
      "Iteration 30: Training Loss = 35.30467773366054, Validation Loss = 16.13361405456608\n",
      "Iteration 31: Training Loss = 35.0116458520068, Validation Loss = 15.969421180798403\n",
      "Iteration 32: Training Loss = 34.72581515218103, Validation Loss = 15.787695608230708\n",
      "Iteration 33: Training Loss = 34.44846265493227, Validation Loss = 15.644613979757208\n",
      "Iteration 34: Training Loss = 34.17323431368439, Validation Loss = 15.47071178769414\n",
      "Iteration 35: Training Loss = 33.89933871517343, Validation Loss = 15.319664243463409\n",
      "Iteration 36: Training Loss = 33.63653222144301, Validation Loss = 15.180864167223358\n",
      "Iteration 37: Training Loss = 33.3770588198611, Validation Loss = 15.039707184845248\n",
      "Iteration 38: Training Loss = 33.11782333884043, Validation Loss = 14.880552319273399\n",
      "Iteration 39: Training Loss = 32.862817169257504, Validation Loss = 14.720946737888994\n",
      "Iteration 40: Training Loss = 32.60937983619647, Validation Loss = 14.586308020553842\n",
      "Iteration 41: Training Loss = 32.36792132669473, Validation Loss = 14.45305162532683\n",
      "Iteration 42: Training Loss = 32.129388651647155, Validation Loss = 14.32392350285908\n",
      "Iteration 43: Training Loss = 31.895610655725257, Validation Loss = 14.197961165054442\n",
      "Iteration 44: Training Loss = 31.665001233364254, Validation Loss = 14.073041776463372\n",
      "Iteration 45: Training Loss = 31.432638607910185, Validation Loss = 13.929542638823762\n",
      "Iteration 46: Training Loss = 31.21072787812288, Validation Loss = 13.810364357909606\n",
      "Iteration 47: Training Loss = 30.992222184085776, Validation Loss = 13.695302258684068\n",
      "Iteration 48: Training Loss = 30.77094999335579, Validation Loss = 13.578846532355906\n",
      "Iteration 49: Training Loss = 30.554645030954703, Validation Loss = 13.4456218674527\n",
      "Iteration 50: Training Loss = 30.347216566150102, Validation Loss = 13.334209030501684\n",
      "Iteration 51: Training Loss = 30.142910466353744, Validation Loss = 13.22243988701735\n",
      "Iteration 52: Training Loss = 29.935975804506676, Validation Loss = 13.100058115621499\n",
      "Iteration 53: Training Loss = 29.737860926733312, Validation Loss = 12.992218885713127\n",
      "Iteration 54: Training Loss = 29.536037925162518, Validation Loss = 12.888742633964743\n",
      "Iteration 55: Training Loss = 29.339210038554665, Validation Loss = 12.766616819746359\n",
      "Iteration 56: Training Loss = 29.151444406703277, Validation Loss = 12.674461407841761\n",
      "Iteration 57: Training Loss = 28.966937731692653, Validation Loss = 12.580471176224947\n",
      "Iteration 58: Training Loss = 28.78520808263347, Validation Loss = 12.483469138873351\n",
      "Iteration 59: Training Loss = 28.60650351104732, Validation Loss = 12.396943587925357\n",
      "Iteration 60: Training Loss = 28.425159183222245, Validation Loss = 12.289348103484553\n",
      "Iteration 61: Training Loss = 28.252836481809833, Validation Loss = 12.20307181247138\n",
      "Iteration 62: Training Loss = 28.075237623613877, Validation Loss = 12.114942925040689\n",
      "Iteration 63: Training Loss = 27.908683773469868, Validation Loss = 12.030730437129655\n",
      "Iteration 64: Training Loss = 27.744264227226942, Validation Loss = 11.947937811969991\n",
      "Iteration 65: Training Loss = 27.582916702300295, Validation Loss = 11.87094431101657\n",
      "Iteration 66: Training Loss = 27.41665650701451, Validation Loss = 11.788914481757404\n",
      "Iteration 67: Training Loss = 27.26043292856497, Validation Loss = 11.710945097121398\n",
      "Iteration 68: Training Loss = 27.1063517974708, Validation Loss = 11.634510888968562\n",
      "Iteration 69: Training Loss = 26.949389640112848, Validation Loss = 11.543116606782716\n",
      "Iteration 70: Training Loss = 26.7937392655588, Validation Loss = 11.466369073184923\n",
      "Iteration 71: Training Loss = 26.647223346163468, Validation Loss = 11.395119631429695\n",
      "Iteration 72: Training Loss = 26.50360336403283, Validation Loss = 11.324848684403719\n",
      "Iteration 73: Training Loss = 26.361614652563595, Validation Loss = 11.253238210457951\n",
      "Iteration 74: Training Loss = 26.215607947470392, Validation Loss = 11.182874103505371\n",
      "Iteration 75: Training Loss = 26.079310213222538, Validation Loss = 11.116077403811842\n",
      "Iteration 76: Training Loss = 25.944847886104736, Validation Loss = 11.053645627927006\n",
      "Iteration 77: Training Loss = 25.811826374000304, Validation Loss = 10.98947240781719\n",
      "Iteration 78: Training Loss = 25.674931074583462, Validation Loss = 10.92207705931043\n",
      "Iteration 79: Training Loss = 25.544742868855234, Validation Loss = 10.859085355107796\n",
      "Iteration 80: Training Loss = 25.417910766643203, Validation Loss = 10.794208253690881\n",
      "Iteration 81: Training Loss = 25.294390103816088, Validation Loss = 10.73100929680139\n",
      "Iteration 82: Training Loss = 25.166327538620553, Validation Loss = 10.65868616093834\n",
      "Iteration 83: Training Loss = 25.04644426975383, Validation Loss = 10.598022446344775\n",
      "Iteration 84: Training Loss = 24.92210097084997, Validation Loss = 10.539630670367467\n",
      "Iteration 85: Training Loss = 24.805661563250744, Validation Loss = 10.482090863036083\n",
      "Iteration 86: Training Loss = 24.690423511404298, Validation Loss = 10.42565943766909\n",
      "Iteration 87: Training Loss = 24.572604755127912, Validation Loss = 10.369682338585068\n",
      "Iteration 88: Training Loss = 24.462209783278382, Validation Loss = 10.31556375191228\n",
      "Iteration 89: Training Loss = 24.353360285240296, Validation Loss = 10.264697631944209\n",
      "Iteration 90: Training Loss = 24.240583260036264, Validation Loss = 10.21248443306164\n",
      "Iteration 91: Training Loss = 24.135055287453916, Validation Loss = 10.163712986142572\n",
      "Iteration 92: Training Loss = 24.03155408411926, Validation Loss = 10.113892653349893\n",
      "Iteration 93: Training Loss = 23.929878297585915, Validation Loss = 10.065983422076071\n",
      "Iteration 94: Training Loss = 23.827484315537923, Validation Loss = 10.021595223451973\n",
      "Iteration 95: Training Loss = 23.72385665543444, Validation Loss = 9.971225466950075\n",
      "Iteration 96: Training Loss = 23.626249312464083, Validation Loss = 9.923555804153159\n",
      "Iteration 97: Training Loss = 23.525381633782093, Validation Loss = 9.877511463600365\n",
      "Iteration 98: Training Loss = 23.430263271366492, Validation Loss = 9.834704375061165\n",
      "Iteration 99: Training Loss = 23.3372607169976, Validation Loss = 9.791945210725276\n",
      "Iteration 100: Training Loss = 23.245982272830307, Validation Loss = 9.748817993139236\n",
      "Iteration 101: Training Loss = 23.151693915839775, Validation Loss = 9.704981715284434\n",
      "Iteration 102: Training Loss = 23.062985394642308, Validation Loss = 9.665718972323715\n",
      "Iteration 103: Training Loss = 22.96643455146899, Validation Loss = 9.619550203277015\n",
      "Iteration 104: Training Loss = 22.880744783393055, Validation Loss = 9.57755901399032\n",
      "Iteration 105: Training Loss = 22.789987368401235, Validation Loss = 9.528586507237465\n",
      "Iteration 106: Training Loss = 22.70655521008902, Validation Loss = 9.490721922556835\n",
      "Iteration 107: Training Loss = 22.615920541397845, Validation Loss = 9.448379296389348\n",
      "Iteration 108: Training Loss = 22.533834679206414, Validation Loss = 9.411127617462979\n",
      "Iteration 109: Training Loss = 22.45482542427054, Validation Loss = 9.375896213006971\n",
      "Iteration 110: Training Loss = 22.37289588009503, Validation Loss = 9.338938629086703\n",
      "Iteration 111: Training Loss = 22.296069129891652, Validation Loss = 9.305265336193889\n",
      "Iteration 112: Training Loss = 22.2206600108204, Validation Loss = 9.271963156049013\n",
      "Iteration 113: Training Loss = 22.136759555278193, Validation Loss = 9.23277464046861\n",
      "Iteration 114: Training Loss = 22.062177579274046, Validation Loss = 9.19876921094839\n",
      "Iteration 115: Training Loss = 21.990534422734946, Validation Loss = 9.167498435481098\n",
      "Iteration 116: Training Loss = 21.9158871730391, Validation Loss = 9.13118520166592\n",
      "Iteration 117: Training Loss = 21.84633000704731, Validation Loss = 9.100578035250884\n",
      "Iteration 118: Training Loss = 21.778016716171912, Validation Loss = 9.071534285015035\n",
      "Iteration 119: Training Loss = 21.701288311856384, Validation Loss = 9.036947891012407\n",
      "Iteration 120: Training Loss = 21.629038035118118, Validation Loss = 9.000198146800122\n",
      "Iteration 121: Training Loss = 21.560023904935893, Validation Loss = 8.967498381164004\n",
      "Iteration 122: Training Loss = 21.494916779807596, Validation Loss = 8.941330176058988\n",
      "Iteration 123: Training Loss = 21.42304918068664, Validation Loss = 8.909685211370059\n",
      "Iteration 124: Training Loss = 21.360716480178652, Validation Loss = 8.881466118611513\n",
      "Iteration 125: Training Loss = 21.297189971867294, Validation Loss = 8.854194957988572\n",
      "Iteration 126: Training Loss = 21.228567947295502, Validation Loss = 8.824122842585147\n",
      "Iteration 127: Training Loss = 21.1691550203239, Validation Loss = 8.797326255566185\n",
      "Iteration 128: Training Loss = 21.107223883136072, Validation Loss = 8.768721178654314\n",
      "Iteration 129: Training Loss = 21.049200045591363, Validation Loss = 8.745210369474323\n",
      "Iteration 130: Training Loss = 20.992049376586845, Validation Loss = 8.722679351187432\n",
      "Iteration 131: Training Loss = 20.93431855746661, Validation Loss = 8.698562092203256\n",
      "Iteration 132: Training Loss = 20.871587809969352, Validation Loss = 8.67215571620759\n",
      "Iteration 133: Training Loss = 20.814360098336866, Validation Loss = 8.646284209033876\n",
      "Iteration 134: Training Loss = 20.75958314102699, Validation Loss = 8.623987474978096\n",
      "Iteration 135: Training Loss = 20.706886211023917, Validation Loss = 8.602804854713941\n",
      "Iteration 136: Training Loss = 20.647914603380574, Validation Loss = 8.578782518698024\n",
      "Iteration 137: Training Loss = 20.593871462472748, Validation Loss = 8.5588172373484\n",
      "Iteration 138: Training Loss = 20.5390917115361, Validation Loss = 8.534191726653946\n",
      "Iteration 139: Training Loss = 20.488346371159327, Validation Loss = 8.514815429015853\n",
      "Iteration 140: Training Loss = 20.433152928651744, Validation Loss = 8.492972185674487\n",
      "Iteration 141: Training Loss = 20.381991873362608, Validation Loss = 8.473628989810049\n",
      "Iteration 142: Training Loss = 20.33330809685717, Validation Loss = 8.454654040153178\n",
      "Iteration 143: Training Loss = 20.28497316101252, Validation Loss = 8.435798481944998\n",
      "Iteration 144: Training Loss = 20.232867682768816, Validation Loss = 8.415404086871616\n",
      "Iteration 145: Training Loss = 20.186736440710444, Validation Loss = 8.398954525276782\n",
      "Iteration 146: Training Loss = 20.137175508880112, Validation Loss = 8.379077430885221\n",
      "Iteration 147: Training Loss = 20.091966590058455, Validation Loss = 8.360918895310652\n",
      "Iteration 148: Training Loss = 20.04811896796953, Validation Loss = 8.34406509825675\n",
      "Iteration 149: Training Loss = 19.999923286145563, Validation Loss = 8.325902094650202\n",
      "Iteration 150: Training Loss = 19.95682269687101, Validation Loss = 8.309387581154303\n",
      "Mean Squared Error on Validation Set: 8.309387581154303\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           4.9284           27.44m\n",
      "         2           4.8947           30.34m\n",
      "         3           4.8617           29.81m\n",
      "         4           4.8293           29.02m\n",
      "         5           4.7975           28.07m\n",
      "         6           4.7664           27.67m\n",
      "         7           4.7358           27.62m\n",
      "         8           4.7057           27.68m\n",
      "         9           4.6758           27.23m\n",
      "        10           4.6464           27.06m\n",
      "        20           4.3821           25.27m\n",
      "        30           4.1540           23.55m\n",
      "        40           3.9613           21.44m\n",
      "        50           3.7986           19.33m\n",
      "        60           3.6600           18.64m\n",
      "        70           3.5413           16.22m\n",
      "        80           3.4391           14.09m\n",
      "        90           3.3510           12.04m\n",
      "       100           3.2731            9.98m\n",
      "Model for store CA_4 saved as model_store_CA_4_GB_0218.joblib\n",
      "Iteration 1: Training Loss = 4.928404665841739, Validation Loss = 3.797357284996829\n",
      "Iteration 2: Training Loss = 4.894727771083441, Validation Loss = 3.769964265577156\n",
      "Iteration 3: Training Loss = 4.861698858366422, Validation Loss = 3.7432276386077836\n",
      "Iteration 4: Training Loss = 4.829315414083734, Validation Loss = 3.716936250935544\n",
      "Iteration 5: Training Loss = 4.797489828791876, Validation Loss = 3.6912112021791286\n",
      "Iteration 6: Training Loss = 4.766404346900941, Validation Loss = 3.665975409054165\n",
      "Iteration 7: Training Loss = 4.735759106141168, Validation Loss = 3.641159317664587\n",
      "Iteration 8: Training Loss = 4.705744112299521, Validation Loss = 3.616813659707498\n",
      "Iteration 9: Training Loss = 4.675811547341208, Validation Loss = 3.5937474394551696\n",
      "Iteration 10: Training Loss = 4.646378259154364, Validation Loss = 3.571319311301311\n",
      "Iteration 11: Training Loss = 4.617843280485843, Validation Loss = 3.5483057950359695\n",
      "Iteration 12: Training Loss = 4.58954434783039, Validation Loss = 3.526579992683747\n",
      "Iteration 13: Training Loss = 4.562063438888132, Validation Loss = 3.504386568365056\n",
      "Iteration 14: Training Loss = 4.535026431507833, Validation Loss = 3.4828108280020866\n",
      "Iteration 15: Training Loss = 4.508208451154041, Validation Loss = 3.462393379501899\n",
      "Iteration 16: Training Loss = 4.482205442193861, Validation Loss = 3.441483639078606\n",
      "Iteration 17: Training Loss = 4.45631391320667, Validation Loss = 3.4217412921376487\n",
      "Iteration 18: Training Loss = 4.431059247022545, Validation Loss = 3.4014432366983405\n",
      "Iteration 19: Training Loss = 4.406484278861958, Validation Loss = 3.3815486056899857\n",
      "Iteration 20: Training Loss = 4.38207739239439, Validation Loss = 3.3620727994712323\n",
      "Iteration 21: Training Loss = 4.357757904792297, Validation Loss = 3.343738283996518\n",
      "Iteration 22: Training Loss = 4.333983768654776, Validation Loss = 3.325586068943894\n",
      "Iteration 23: Training Loss = 4.309908615084558, Validation Loss = 3.306777208862897\n",
      "Iteration 24: Training Loss = 4.287051201741292, Validation Loss = 3.2886684361525713\n",
      "Iteration 25: Training Loss = 4.263853715259904, Validation Loss = 3.2702372646203988\n",
      "Iteration 26: Training Loss = 4.241109724658027, Validation Loss = 3.252571564071451\n",
      "Iteration 27: Training Loss = 4.21873517581273, Validation Loss = 3.2351473541495492\n",
      "Iteration 28: Training Loss = 4.196825288321314, Validation Loss = 3.217539395063079\n",
      "Iteration 29: Training Loss = 4.175288262088728, Validation Loss = 3.200487181252085\n",
      "Iteration 30: Training Loss = 4.1539690493571735, Validation Loss = 3.1841391919168074\n",
      "Iteration 31: Training Loss = 4.133251185979752, Validation Loss = 3.1684138170871115\n",
      "Iteration 32: Training Loss = 4.112673994528718, Validation Loss = 3.1524787074901113\n",
      "Iteration 33: Training Loss = 4.092630729973602, Validation Loss = 3.1377707563222454\n",
      "Iteration 34: Training Loss = 4.072953741772726, Validation Loss = 3.1233965659614573\n",
      "Iteration 35: Training Loss = 4.053570208377764, Validation Loss = 3.1081940200951825\n",
      "Iteration 36: Training Loss = 4.034523162665821, Validation Loss = 3.093938932968483\n",
      "Iteration 37: Training Loss = 4.015840604789919, Validation Loss = 3.0803282253351916\n",
      "Iteration 38: Training Loss = 3.9972840841945505, Validation Loss = 3.066145382568549\n",
      "Iteration 39: Training Loss = 3.9790997798741237, Validation Loss = 3.0519511885692703\n",
      "Iteration 40: Training Loss = 3.9613047514142186, Validation Loss = 3.038741988199721\n",
      "Iteration 41: Training Loss = 3.9437586083923417, Validation Loss = 3.0262456575692083\n",
      "Iteration 42: Training Loss = 3.92660315128069, Validation Loss = 3.0134448347190177\n",
      "Iteration 43: Training Loss = 3.909700336467859, Validation Loss = 3.0002689796934496\n",
      "Iteration 44: Training Loss = 3.893014206764461, Validation Loss = 2.987675661051011\n",
      "Iteration 45: Training Loss = 3.876564311476006, Validation Loss = 2.975917658524563\n",
      "Iteration 46: Training Loss = 3.8603893238100215, Validation Loss = 2.9643786698564223\n",
      "Iteration 47: Training Loss = 3.844534458670116, Validation Loss = 2.952441959815803\n",
      "Iteration 48: Training Loss = 3.829015407274647, Validation Loss = 2.9411157916869017\n",
      "Iteration 49: Training Loss = 3.8136371850201356, Validation Loss = 2.9294053797148036\n",
      "Iteration 50: Training Loss = 3.798554864595056, Validation Loss = 2.918218527069293\n",
      "Iteration 51: Training Loss = 3.7835907064842025, Validation Loss = 2.907594364118163\n",
      "Iteration 52: Training Loss = 3.768938463739797, Validation Loss = 2.8974148592713127\n",
      "Iteration 53: Training Loss = 3.754475792395257, Validation Loss = 2.8865766890992273\n",
      "Iteration 54: Training Loss = 3.7403291390218714, Validation Loss = 2.8760709183836903\n",
      "Iteration 55: Training Loss = 3.726400749610869, Validation Loss = 2.8655674146563217\n",
      "Iteration 56: Training Loss = 3.712657752473481, Validation Loss = 2.8549516444279543\n",
      "Iteration 57: Training Loss = 3.6991328465556004, Validation Loss = 2.8445773392260887\n",
      "Iteration 58: Training Loss = 3.6859300326338436, Validation Loss = 2.834331170730368\n",
      "Iteration 59: Training Loss = 3.6728712058289865, Validation Loss = 2.82435931834918\n",
      "Iteration 60: Training Loss = 3.659991584889396, Validation Loss = 2.814728172402601\n",
      "Iteration 61: Training Loss = 3.647335918927464, Validation Loss = 2.805757721981348\n",
      "Iteration 62: Training Loss = 3.63486983580413, Validation Loss = 2.7966993274124987\n",
      "Iteration 63: Training Loss = 3.6224059822163976, Validation Loss = 2.7879072221382026\n",
      "Iteration 64: Training Loss = 3.6102746129639316, Validation Loss = 2.779570285031534\n",
      "Iteration 65: Training Loss = 3.5982584265145845, Validation Loss = 2.770653924300368\n",
      "Iteration 66: Training Loss = 3.58648671378969, Validation Loss = 2.7619859667266033\n",
      "Iteration 67: Training Loss = 3.5749253053284926, Validation Loss = 2.753391117847143\n",
      "Iteration 68: Training Loss = 3.563532856545445, Validation Loss = 2.7446684807913835\n",
      "Iteration 69: Training Loss = 3.5522980687427297, Validation Loss = 2.7361492722008895\n",
      "Iteration 70: Training Loss = 3.5412651289339547, Validation Loss = 2.728381323980532\n",
      "Iteration 71: Training Loss = 3.5303907742954403, Validation Loss = 2.7202185356047237\n",
      "Iteration 72: Training Loss = 3.5195993756151958, Validation Loss = 2.712716268464561\n",
      "Iteration 73: Training Loss = 3.508866363025158, Validation Loss = 2.7051802180904687\n",
      "Iteration 74: Training Loss = 3.4985371192943906, Validation Loss = 2.697337813278914\n",
      "Iteration 75: Training Loss = 3.488235213792507, Validation Loss = 2.6899491183034745\n",
      "Iteration 76: Training Loss = 3.4780966664145767, Validation Loss = 2.6825571528300136\n",
      "Iteration 77: Training Loss = 3.4681064649451803, Validation Loss = 2.6754830866972616\n",
      "Iteration 78: Training Loss = 3.458276482065342, Validation Loss = 2.668581919384653\n",
      "Iteration 79: Training Loss = 3.448620714705697, Validation Loss = 2.6613398928895395\n",
      "Iteration 80: Training Loss = 3.4390991039789007, Validation Loss = 2.654602181349969\n",
      "Iteration 81: Training Loss = 3.4296653455686754, Validation Loss = 2.6477933558232314\n",
      "Iteration 82: Training Loss = 3.4204284550813666, Validation Loss = 2.6407384376640475\n",
      "Iteration 83: Training Loss = 3.4113464157993865, Validation Loss = 2.634123267984445\n",
      "Iteration 84: Training Loss = 3.40242265561929, Validation Loss = 2.6280681793076566\n",
      "Iteration 85: Training Loss = 3.3935428482722263, Validation Loss = 2.621630370316711\n",
      "Iteration 86: Training Loss = 3.3847035700433503, Validation Loss = 2.6154503907419295\n",
      "Iteration 87: Training Loss = 3.376110511570887, Validation Loss = 2.6090101341588516\n",
      "Iteration 88: Training Loss = 3.367589019382028, Validation Loss = 2.602964990304849\n",
      "Iteration 89: Training Loss = 3.3592450315857705, Validation Loss = 2.596961140049434\n",
      "Iteration 90: Training Loss = 3.35102990821454, Validation Loss = 2.591225625023332\n",
      "Iteration 91: Training Loss = 3.342975798153593, Validation Loss = 2.5854608525268956\n",
      "Iteration 92: Training Loss = 3.334999677676698, Validation Loss = 2.5799999106221354\n",
      "Iteration 93: Training Loss = 3.3263867543140178, Validation Loss = 2.5743827265088446\n",
      "Iteration 94: Training Loss = 3.3186171192230662, Validation Loss = 2.569141860210995\n",
      "Iteration 95: Training Loss = 3.3109589667205492, Validation Loss = 2.5635276814803207\n",
      "Iteration 96: Training Loss = 3.3034572341532336, Validation Loss = 2.5584270499728285\n",
      "Iteration 97: Training Loss = 3.2953343105432675, Validation Loss = 2.553206028260034\n",
      "Iteration 98: Training Loss = 3.2880532974508743, Validation Loss = 2.548063944661071\n",
      "Iteration 99: Training Loss = 3.280837806919714, Validation Loss = 2.542978112603579\n",
      "Iteration 100: Training Loss = 3.273066326809503, Validation Loss = 2.5379065497121447\n",
      "Iteration 101: Training Loss = 3.2661166625872955, Validation Loss = 2.532740284597639\n",
      "Iteration 102: Training Loss = 3.259244201105854, Validation Loss = 2.527643474568515\n",
      "Iteration 103: Training Loss = 3.2518059816020455, Validation Loss = 2.522838859021702\n",
      "Iteration 104: Training Loss = 3.245126704341576, Validation Loss = 2.518259805228117\n",
      "Iteration 105: Training Loss = 3.238461276694956, Validation Loss = 2.5135986753068345\n",
      "Iteration 106: Training Loss = 3.231928555307304, Validation Loss = 2.5092444066118937\n",
      "Iteration 107: Training Loss = 3.2248390749541374, Validation Loss = 2.5047667530278055\n",
      "Iteration 108: Training Loss = 3.218453174413464, Validation Loss = 2.500714486319854\n",
      "Iteration 109: Training Loss = 3.2122422723654087, Validation Loss = 2.4963872919239707\n",
      "Iteration 110: Training Loss = 3.2054411037118826, Validation Loss = 2.492219830373607\n",
      "Iteration 111: Training Loss = 3.1993427913293484, Validation Loss = 2.487898571118421\n",
      "Iteration 112: Training Loss = 3.1933722334843777, Validation Loss = 2.483816149148894\n",
      "Iteration 113: Training Loss = 3.187374942129031, Validation Loss = 2.48006530003758\n",
      "Iteration 114: Training Loss = 3.180923733078143, Validation Loss = 2.476022247826256\n",
      "Iteration 115: Training Loss = 3.1751378537663815, Validation Loss = 2.4722720207486053\n",
      "Iteration 116: Training Loss = 3.1695060588100064, Validation Loss = 2.4683953982994504\n",
      "Iteration 117: Training Loss = 3.163912451950894, Validation Loss = 2.464805640453533\n",
      "Iteration 118: Training Loss = 3.1583378661193358, Validation Loss = 2.461382046386467\n",
      "Iteration 119: Training Loss = 3.1523696288819654, Validation Loss = 2.4574907043445524\n",
      "Iteration 120: Training Loss = 3.1470024520589073, Validation Loss = 2.453579787832998\n",
      "Iteration 121: Training Loss = 3.1416637266193277, Validation Loss = 2.45024460619958\n",
      "Iteration 122: Training Loss = 3.136473718284034, Validation Loss = 2.446734873017115\n",
      "Iteration 123: Training Loss = 3.130764321141257, Validation Loss = 2.4433393609324794\n",
      "Iteration 124: Training Loss = 3.1255085667071802, Validation Loss = 2.4399934322988517\n",
      "Iteration 125: Training Loss = 3.120493052725587, Validation Loss = 2.4367012470491596\n",
      "Iteration 126: Training Loss = 3.1155707272516335, Validation Loss = 2.433361973587364\n",
      "Iteration 127: Training Loss = 3.110164942373298, Validation Loss = 2.4301931197114346\n",
      "Iteration 128: Training Loss = 3.1053463107237262, Validation Loss = 2.4272944373832335\n",
      "Iteration 129: Training Loss = 3.1006160987307716, Validation Loss = 2.424308070846541\n",
      "Iteration 130: Training Loss = 3.09591944851604, Validation Loss = 2.4212152889579825\n",
      "Iteration 131: Training Loss = 3.0913077853796374, Validation Loss = 2.4182479171077196\n",
      "Iteration 132: Training Loss = 3.0863047603294143, Validation Loss = 2.415208528583597\n",
      "Iteration 133: Training Loss = 3.0818179979901585, Validation Loss = 2.412108394562233\n",
      "Iteration 134: Training Loss = 3.0773778456885177, Validation Loss = 2.4093680143634946\n",
      "Iteration 135: Training Loss = 3.0728557039716686, Validation Loss = 2.406125245568115\n",
      "Iteration 136: Training Loss = 3.068541262683449, Validation Loss = 2.403392516353212\n",
      "Iteration 137: Training Loss = 3.0638718727827072, Validation Loss = 2.4005076928727154\n",
      "Iteration 138: Training Loss = 3.059722273853592, Validation Loss = 2.397851350725444\n",
      "Iteration 139: Training Loss = 3.05561693345558, Validation Loss = 2.3950771187635445\n",
      "Iteration 140: Training Loss = 3.051529916900006, Validation Loss = 2.3924992900799986\n",
      "Iteration 141: Training Loss = 3.0473683519002823, Validation Loss = 2.3898821812877546\n",
      "Iteration 142: Training Loss = 3.0434208458919025, Validation Loss = 2.3874016394004953\n",
      "Iteration 143: Training Loss = 3.0390415949847687, Validation Loss = 2.384767303974108\n",
      "Iteration 144: Training Loss = 3.035181104770062, Validation Loss = 2.3821348366435138\n",
      "Iteration 145: Training Loss = 3.0313533580965046, Validation Loss = 2.3798843380676837\n",
      "Iteration 146: Training Loss = 3.0276038881884744, Validation Loss = 2.377567972140332\n",
      "Iteration 147: Training Loss = 3.023509863802925, Validation Loss = 2.3751165420901623\n",
      "Iteration 148: Training Loss = 3.019880157432434, Validation Loss = 2.372828867146347\n",
      "Iteration 149: Training Loss = 3.0162659755684684, Validation Loss = 2.370586699958712\n",
      "Iteration 150: Training Loss = 3.012739317057042, Validation Loss = 2.3684355114946904\n",
      "Mean Squared Error on Validation Set: 2.3684355114946904\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1          13.3163           32.41m\n",
      "         2          13.1872           29.35m\n",
      "         3          13.0606           27.96m\n",
      "         4          12.9365           28.18m\n",
      "         5          12.8149           31.51m\n",
      "         6          12.6957           33.64m\n",
      "         7          12.5786           34.94m\n",
      "         8          12.4639           35.85m\n",
      "         9          12.3511           36.24m\n",
      "        10          12.2407           36.67m\n",
      "        20          11.2375           36.04m\n",
      "        30          10.3986           34.19m\n",
      "        40           9.6964           31.95m\n",
      "        50           9.1060           27.36m\n",
      "        60           8.6107           23.18m\n",
      "        70           8.1927           19.70m\n",
      "        80           7.8371           16.67m\n",
      "        90           7.5340           14.63m\n",
      "       100           7.2754           11.78m\n",
      "Model for store TX_1 saved as model_store_TX_1_GB_0218.joblib\n",
      "Iteration 1: Training Loss = 13.3163190149366, Validation Loss = 9.63014718750238\n",
      "Iteration 2: Training Loss = 13.187190133894573, Validation Loss = 9.527091329427192\n",
      "Iteration 3: Training Loss = 13.060593472230753, Validation Loss = 9.425483017253573\n",
      "Iteration 4: Training Loss = 12.936509285368492, Validation Loss = 9.326553586887616\n",
      "Iteration 5: Training Loss = 12.814850791175383, Validation Loss = 9.228928288630252\n",
      "Iteration 6: Training Loss = 12.695736664108617, Validation Loss = 9.137385764153796\n",
      "Iteration 7: Training Loss = 12.578594954500838, Validation Loss = 9.043620310666224\n",
      "Iteration 8: Training Loss = 12.463925774306958, Validation Loss = 8.955821996472045\n",
      "Iteration 9: Training Loss = 12.351107527961789, Validation Loss = 8.865918074893072\n",
      "Iteration 10: Training Loss = 12.240663483545093, Validation Loss = 8.781710379079374\n",
      "Iteration 11: Training Loss = 12.132002947320766, Validation Loss = 8.694799379496787\n",
      "Iteration 12: Training Loss = 12.025416983806428, Validation Loss = 8.610086568394875\n",
      "Iteration 13: Training Loss = 11.92062658057582, Validation Loss = 8.531769338968205\n",
      "Iteration 14: Training Loss = 11.817069435658404, Validation Loss = 8.450740466829494\n",
      "Iteration 15: Training Loss = 11.716425325963433, Validation Loss = 8.373869878131053\n",
      "Iteration 16: Training Loss = 11.61735640929118, Validation Loss = 8.29553572776579\n",
      "Iteration 17: Training Loss = 11.519478269482633, Validation Loss = 8.218780699767214\n",
      "Iteration 18: Training Loss = 11.424044132458274, Validation Loss = 8.147465209151083\n",
      "Iteration 19: Training Loss = 11.330324694764272, Validation Loss = 8.072845109378388\n",
      "Iteration 20: Training Loss = 11.237491596003451, Validation Loss = 7.993077682715851\n",
      "Iteration 21: Training Loss = 11.14717134593773, Validation Loss = 7.921126803054708\n",
      "Iteration 22: Training Loss = 11.057741984722135, Validation Loss = 7.843186988383529\n",
      "Iteration 23: Training Loss = 10.969986497908756, Validation Loss = 7.77456519539104\n",
      "Iteration 24: Training Loss = 10.883739186107015, Validation Loss = 7.70040872803278\n",
      "Iteration 25: Training Loss = 10.799154160229453, Validation Loss = 7.6276945529484665\n",
      "Iteration 26: Training Loss = 10.716529465238846, Validation Loss = 7.562660514713878\n",
      "Iteration 27: Training Loss = 10.634745847686727, Validation Loss = 7.4914051952689995\n",
      "Iteration 28: Training Loss = 10.554719948441853, Validation Loss = 7.421403606687078\n",
      "Iteration 29: Training Loss = 10.476116347693885, Validation Loss = 7.353144881829189\n",
      "Iteration 30: Training Loss = 10.398570786573165, Validation Loss = 7.29183065898102\n",
      "Iteration 31: Training Loss = 10.322153821663582, Validation Loss = 7.232711996752644\n",
      "Iteration 32: Training Loss = 10.247667307322647, Validation Loss = 7.168725263367145\n",
      "Iteration 33: Training Loss = 10.174577887165146, Validation Loss = 7.104774244133408\n",
      "Iteration 34: Training Loss = 10.102407411188494, Validation Loss = 7.047464401903946\n",
      "Iteration 35: Training Loss = 10.031557674382677, Validation Loss = 6.989176120703445\n",
      "Iteration 36: Training Loss = 9.96230355643456, Validation Loss = 6.931242274678648\n",
      "Iteration 37: Training Loss = 9.894159807931477, Validation Loss = 6.87169500995066\n",
      "Iteration 38: Training Loss = 9.827349425673896, Validation Loss = 6.816182461082841\n",
      "Iteration 39: Training Loss = 9.760805195245263, Validation Loss = 6.761050117307369\n",
      "Iteration 40: Training Loss = 9.69636256950515, Validation Loss = 6.707175420541954\n",
      "Iteration 41: Training Loss = 9.632414631928397, Validation Loss = 6.6561868536650834\n",
      "Iteration 42: Training Loss = 9.569217732954238, Validation Loss = 6.607448495032245\n",
      "Iteration 43: Training Loss = 9.50811152495577, Validation Loss = 6.556207430696684\n",
      "Iteration 44: Training Loss = 9.448149301658875, Validation Loss = 6.505896465138168\n",
      "Iteration 45: Training Loss = 9.388048073750424, Validation Loss = 6.454351773226898\n",
      "Iteration 46: Training Loss = 9.32994177626502, Validation Loss = 6.403552441473391\n",
      "Iteration 47: Training Loss = 9.272247871242644, Validation Loss = 6.357471946367753\n",
      "Iteration 48: Training Loss = 9.216219917629273, Validation Loss = 6.310479596025235\n",
      "Iteration 49: Training Loss = 9.160130089785453, Validation Loss = 6.263159227023234\n",
      "Iteration 50: Training Loss = 9.105986901573317, Validation Loss = 6.217329244444926\n",
      "Iteration 51: Training Loss = 9.052622329232388, Validation Loss = 6.171402040398387\n",
      "Iteration 52: Training Loss = 9.000432241811305, Validation Loss = 6.1271959808584775\n",
      "Iteration 53: Training Loss = 8.948094534157082, Validation Loss = 6.086277785344282\n",
      "Iteration 54: Training Loss = 8.897797660277654, Validation Loss = 6.044240792212449\n",
      "Iteration 55: Training Loss = 8.847094135641742, Validation Loss = 6.002935696962569\n",
      "Iteration 56: Training Loss = 8.798468779574439, Validation Loss = 5.95838978213131\n",
      "Iteration 57: Training Loss = 8.750509902052714, Validation Loss = 5.918968236544986\n",
      "Iteration 58: Training Loss = 8.703323005090432, Validation Loss = 5.8778450406455995\n",
      "Iteration 59: Training Loss = 8.655989116766055, Validation Loss = 5.841138987903095\n",
      "Iteration 60: Training Loss = 8.610672461016689, Validation Loss = 5.800861652310676\n",
      "Iteration 61: Training Loss = 8.566255595802136, Validation Loss = 5.760518121810757\n",
      "Iteration 62: Training Loss = 8.52115668120141, Validation Loss = 5.724475505320393\n",
      "Iteration 63: Training Loss = 8.477715031337063, Validation Loss = 5.6851923787226255\n",
      "Iteration 64: Training Loss = 8.434296610906095, Validation Loss = 5.650743270479337\n",
      "Iteration 65: Training Loss = 8.392321219810412, Validation Loss = 5.614082374827817\n",
      "Iteration 66: Training Loss = 8.350945362481326, Validation Loss = 5.58065453531269\n",
      "Iteration 67: Training Loss = 8.310540674181443, Validation Loss = 5.544726939119835\n",
      "Iteration 68: Training Loss = 8.270554143131296, Validation Loss = 5.510866139100928\n",
      "Iteration 69: Training Loss = 8.231364335163919, Validation Loss = 5.47581411704304\n",
      "Iteration 70: Training Loss = 8.192694242202103, Validation Loss = 5.443032716370292\n",
      "Iteration 71: Training Loss = 8.153728392278165, Validation Loss = 5.411990677198009\n",
      "Iteration 72: Training Loss = 8.116576875188677, Validation Loss = 5.378184626256769\n",
      "Iteration 73: Training Loss = 8.080048969343196, Validation Loss = 5.3457059757348855\n",
      "Iteration 74: Training Loss = 8.042923560336579, Validation Loss = 5.3159125664927895\n",
      "Iteration 75: Training Loss = 8.00769668555111, Validation Loss = 5.283971584170585\n",
      "Iteration 76: Training Loss = 7.97298029327321, Validation Loss = 5.252614990590282\n",
      "Iteration 77: Training Loss = 7.937759941655853, Validation Loss = 5.2254501243637215\n",
      "Iteration 78: Training Loss = 7.902692097695951, Validation Loss = 5.197311727922562\n",
      "Iteration 79: Training Loss = 7.869582422494586, Validation Loss = 5.169028845677362\n",
      "Iteration 80: Training Loss = 7.837132129361884, Validation Loss = 5.141564691619462\n",
      "Iteration 81: Training Loss = 7.805210929960751, Validation Loss = 5.112458795299265\n",
      "Iteration 82: Training Loss = 7.772716980709627, Validation Loss = 5.086536640826853\n",
      "Iteration 83: Training Loss = 7.740705557961553, Validation Loss = 5.0618890873524185\n",
      "Iteration 84: Training Loss = 7.709906988668722, Validation Loss = 5.037784865910882\n",
      "Iteration 85: Training Loss = 7.679906368964463, Validation Loss = 5.0108493687085724\n",
      "Iteration 86: Training Loss = 7.650474051869618, Validation Loss = 4.984315799260686\n",
      "Iteration 87: Training Loss = 7.621409311202441, Validation Loss = 4.9590580865449185\n",
      "Iteration 88: Training Loss = 7.590573743826798, Validation Loss = 4.935369208438707\n",
      "Iteration 89: Training Loss = 7.562594518421035, Validation Loss = 4.911105042416625\n",
      "Iteration 90: Training Loss = 7.534005457860573, Validation Loss = 4.888481175566008\n",
      "Iteration 91: Training Loss = 7.5069085029580025, Validation Loss = 4.8652885940566\n",
      "Iteration 92: Training Loss = 7.480193939305059, Validation Loss = 4.843200903846699\n",
      "Iteration 93: Training Loss = 7.4517079882739745, Validation Loss = 4.82128510996921\n",
      "Iteration 94: Training Loss = 7.425747120032375, Validation Loss = 4.798751198739219\n",
      "Iteration 95: Training Loss = 7.40041526901676, Validation Loss = 4.776566535721941\n",
      "Iteration 96: Training Loss = 7.375359874911992, Validation Loss = 4.753636271522419\n",
      "Iteration 97: Training Loss = 7.34860846936508, Validation Loss = 4.733251757225961\n",
      "Iteration 98: Training Loss = 7.322752635021175, Validation Loss = 4.713850905043786\n",
      "Iteration 99: Training Loss = 7.29884895180563, Validation Loss = 4.693604745898198\n",
      "Iteration 100: Training Loss = 7.275359603539102, Validation Loss = 4.672552310657795\n",
      "Iteration 101: Training Loss = 7.252209305004754, Validation Loss = 4.653193923035048\n",
      "Iteration 102: Training Loss = 7.229369184364164, Validation Loss = 4.633583129925316\n",
      "Iteration 103: Training Loss = 7.206949039537633, Validation Loss = 4.614505106276328\n",
      "Iteration 104: Training Loss = 7.182940516310221, Validation Loss = 4.5961307159416736\n",
      "Iteration 105: Training Loss = 7.16128353778822, Validation Loss = 4.577868948944384\n",
      "Iteration 106: Training Loss = 7.139865033281791, Validation Loss = 4.559394952351416\n",
      "Iteration 107: Training Loss = 7.117297774972144, Validation Loss = 4.542353828614439\n",
      "Iteration 108: Training Loss = 7.096574901966445, Validation Loss = 4.524205865351244\n",
      "Iteration 109: Training Loss = 7.0761918535021335, Validation Loss = 4.507471391991999\n",
      "Iteration 110: Training Loss = 7.056108563204535, Validation Loss = 4.490089758478069\n",
      "Iteration 111: Training Loss = 7.034572411166512, Validation Loss = 4.473659550300775\n",
      "Iteration 112: Training Loss = 7.015169267630596, Validation Loss = 4.457583076246802\n",
      "Iteration 113: Training Loss = 6.995150313510549, Validation Loss = 4.441764396592024\n",
      "Iteration 114: Training Loss = 6.976266460623231, Validation Loss = 4.42563047937261\n",
      "Iteration 115: Training Loss = 6.956858945384336, Validation Loss = 4.411487193979726\n",
      "Iteration 116: Training Loss = 6.938649753565322, Validation Loss = 4.396083802886661\n",
      "Iteration 117: Training Loss = 6.920653912632894, Validation Loss = 4.380471813599539\n",
      "Iteration 118: Training Loss = 6.90130334767061, Validation Loss = 4.365946393615053\n",
      "Iteration 119: Training Loss = 6.883900169814638, Validation Loss = 4.350964442977969\n",
      "Iteration 120: Training Loss = 6.866140394483003, Validation Loss = 4.337108376612872\n",
      "Iteration 121: Training Loss = 6.849333703705062, Validation Loss = 4.322546046771792\n",
      "Iteration 122: Training Loss = 6.831938563247877, Validation Loss = 4.310772361570998\n",
      "Iteration 123: Training Loss = 6.815619399510745, Validation Loss = 4.2968304784920885\n",
      "Iteration 124: Training Loss = 6.799643704073913, Validation Loss = 4.283564854732124\n",
      "Iteration 125: Training Loss = 6.782608911314697, Validation Loss = 4.270482235731204\n",
      "Iteration 126: Training Loss = 6.767083838654561, Validation Loss = 4.256112089179089\n",
      "Iteration 127: Training Loss = 6.750238952213429, Validation Loss = 4.243563853039184\n",
      "Iteration 128: Training Loss = 6.735179028252113, Validation Loss = 4.231260505888736\n",
      "Iteration 129: Training Loss = 6.720209499576789, Validation Loss = 4.218302835107255\n",
      "Iteration 130: Training Loss = 6.704330175064634, Validation Loss = 4.206483827736499\n",
      "Iteration 131: Training Loss = 6.689942961490185, Validation Loss = 4.193442753907311\n",
      "Iteration 132: Training Loss = 6.675712736849526, Validation Loss = 4.1816750520857955\n",
      "Iteration 133: Training Loss = 6.660299364477462, Validation Loss = 4.170171480456511\n",
      "Iteration 134: Training Loss = 6.646475912031281, Validation Loss = 4.1583810350671415\n",
      "Iteration 135: Training Loss = 6.632958298924442, Validation Loss = 4.14665639409945\n",
      "Iteration 136: Training Loss = 6.619694389830859, Validation Loss = 4.134621806443153\n",
      "Iteration 137: Training Loss = 6.606059491509236, Validation Loss = 4.124038742353676\n",
      "Iteration 138: Training Loss = 6.59307655978929, Validation Loss = 4.113235896867568\n",
      "Iteration 139: Training Loss = 6.580358540510383, Validation Loss = 4.101673116753913\n",
      "Iteration 140: Training Loss = 6.566498152675866, Validation Loss = 4.091417657054765\n",
      "Iteration 141: Training Loss = 6.55419933428074, Validation Loss = 4.080683632164704\n",
      "Iteration 142: Training Loss = 6.5420993245046635, Validation Loss = 4.070180996654475\n",
      "Iteration 143: Training Loss = 6.529454180496467, Validation Loss = 4.061398457360322\n",
      "Iteration 144: Training Loss = 6.516690667573723, Validation Loss = 4.0517093697291715\n",
      "Iteration 145: Training Loss = 6.505181051329568, Validation Loss = 4.042183430515668\n",
      "Iteration 146: Training Loss = 6.493829678633733, Validation Loss = 4.031712643703771\n",
      "Iteration 147: Training Loss = 6.4813570279090635, Validation Loss = 4.022571152438905\n",
      "Iteration 148: Training Loss = 6.469308894232917, Validation Loss = 4.01342972000658\n",
      "Iteration 149: Training Loss = 6.458499268255407, Validation Loss = 4.004327042400089\n",
      "Iteration 150: Training Loss = 6.446593548979496, Validation Loss = 3.9955226398796326\n",
      "Mean Squared Error on Validation Set: 3.9955226398796326\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1          23.5084           48.40m\n",
      "         2          23.2453           47.99m\n",
      "         3          22.9874           47.56m\n",
      "         4          22.7346           47.24m\n",
      "         5          22.4867           47.09m\n",
      "         6          22.2438           47.02m\n",
      "         7          22.0053           46.71m\n",
      "         8          21.7715           46.42m\n",
      "         9          21.5425           46.07m\n",
      "        10          21.3178           45.73m\n",
      "        20          19.2867           42.32m\n",
      "        30          17.5968           39.08m\n",
      "        40          16.1833           35.87m\n",
      "        50          14.9791           32.69m\n",
      "        60          13.9637           29.41m\n",
      "        70          13.1044           25.87m\n",
      "        80          12.3807           21.24m\n",
      "        90          11.7718           17.28m\n",
      "       100          11.2593           13.78m\n",
      "Model for store TX_2 saved as model_store_TX_2_GB_0218.joblib\n",
      "Iteration 1: Training Loss = 23.508388169492832, Validation Loss = 13.535266369631838\n",
      "Iteration 2: Training Loss = 23.245281134384275, Validation Loss = 13.366851428389444\n",
      "Iteration 3: Training Loss = 22.987370237824106, Validation Loss = 13.201889457310061\n",
      "Iteration 4: Training Loss = 22.734586532936298, Validation Loss = 13.040308900218365\n",
      "Iteration 5: Training Loss = 22.48671040506646, Validation Loss = 12.88239780019925\n",
      "Iteration 6: Training Loss = 22.243755435730645, Validation Loss = 12.728354154919668\n",
      "Iteration 7: Training Loss = 22.00529445009721, Validation Loss = 12.575882710263738\n",
      "Iteration 8: Training Loss = 21.77152762069676, Validation Loss = 12.428461314164696\n",
      "Iteration 9: Training Loss = 21.542516507492405, Validation Loss = 12.282713640364284\n",
      "Iteration 10: Training Loss = 21.317782685449743, Validation Loss = 12.140640197649198\n",
      "Iteration 11: Training Loss = 21.096630779927896, Validation Loss = 11.996731639449468\n",
      "Iteration 12: Training Loss = 20.880403853596853, Validation Loss = 11.859399298631017\n",
      "Iteration 13: Training Loss = 20.66749639240418, Validation Loss = 11.71975444108148\n",
      "Iteration 14: Training Loss = 20.459115972339116, Validation Loss = 11.587149804176363\n",
      "Iteration 15: Training Loss = 20.254230851830506, Validation Loss = 11.453526190855532\n",
      "Iteration 16: Training Loss = 20.053718038267434, Validation Loss = 11.325645163826144\n",
      "Iteration 17: Training Loss = 19.856447220822954, Validation Loss = 11.194258186970512\n",
      "Iteration 18: Training Loss = 19.663001612560976, Validation Loss = 11.070757550844636\n",
      "Iteration 19: Training Loss = 19.473009807054748, Validation Loss = 10.946974557216501\n",
      "Iteration 20: Training Loss = 19.286724410420035, Validation Loss = 10.825241079213692\n",
      "Iteration 21: Training Loss = 19.104050912251658, Validation Loss = 10.710698218856498\n",
      "Iteration 22: Training Loss = 18.92425682667405, Validation Loss = 10.593904148814817\n",
      "Iteration 23: Training Loss = 18.747709763986432, Validation Loss = 10.481052487034095\n",
      "Iteration 24: Training Loss = 18.57437137723871, Validation Loss = 10.364387020029207\n",
      "Iteration 25: Training Loss = 18.40432892447263, Validation Loss = 10.254496036430512\n",
      "Iteration 26: Training Loss = 18.23747432772923, Validation Loss = 10.149009205247188\n",
      "Iteration 27: Training Loss = 18.07339560373653, Validation Loss = 10.043727304037953\n",
      "Iteration 28: Training Loss = 17.9112699086168, Validation Loss = 9.940424978992823\n",
      "Iteration 29: Training Loss = 17.75318408344282, Validation Loss = 9.833572210462878\n",
      "Iteration 30: Training Loss = 17.596803707174352, Validation Loss = 9.73410449604395\n",
      "Iteration 31: Training Loss = 17.443154663380913, Validation Loss = 9.635268493779147\n",
      "Iteration 32: Training Loss = 17.293129028498896, Validation Loss = 9.53377332329524\n",
      "Iteration 33: Training Loss = 17.14504009392418, Validation Loss = 9.437057552831964\n",
      "Iteration 34: Training Loss = 17.000592362269913, Validation Loss = 9.34394911240468\n",
      "Iteration 35: Training Loss = 16.85858444051299, Validation Loss = 9.254283090233047\n",
      "Iteration 36: Training Loss = 16.7181121202165, Validation Loss = 9.16504905030051\n",
      "Iteration 37: Training Loss = 16.580310432870277, Validation Loss = 9.076098115392725\n",
      "Iteration 38: Training Loss = 16.445550845791455, Validation Loss = 8.991842470376143\n",
      "Iteration 39: Training Loss = 16.313236720958503, Validation Loss = 8.906134439543715\n",
      "Iteration 40: Training Loss = 16.18326312042275, Validation Loss = 8.826430802944508\n",
      "Iteration 41: Training Loss = 16.055347933150035, Validation Loss = 8.740852213690363\n",
      "Iteration 42: Training Loss = 15.925965382936504, Validation Loss = 8.652825894389395\n",
      "Iteration 43: Training Loss = 15.799149043995993, Validation Loss = 8.566776090988984\n",
      "Iteration 44: Training Loss = 15.678135256662786, Validation Loss = 8.486219607416777\n",
      "Iteration 45: Training Loss = 15.55583010151902, Validation Loss = 8.403575192646858\n",
      "Iteration 46: Training Loss = 15.435928397505116, Validation Loss = 8.322404350549393\n",
      "Iteration 47: Training Loss = 15.317430262425217, Validation Loss = 8.242205620754746\n",
      "Iteration 48: Training Loss = 15.204412194644261, Validation Loss = 8.170459670280637\n",
      "Iteration 49: Training Loss = 15.090161689031545, Validation Loss = 8.093142606701065\n",
      "Iteration 50: Training Loss = 14.979090274820486, Validation Loss = 8.01789774830016\n",
      "Iteration 51: Training Loss = 14.86901040089693, Validation Loss = 7.943704577030671\n",
      "Iteration 52: Training Loss = 14.763588588145877, Validation Loss = 7.874083230784892\n",
      "Iteration 53: Training Loss = 14.65742627320735, Validation Loss = 7.802694210359836\n",
      "Iteration 54: Training Loss = 14.554066545679587, Validation Loss = 7.732633469725268\n",
      "Iteration 55: Training Loss = 14.45174336802501, Validation Loss = 7.664135367033927\n",
      "Iteration 56: Training Loss = 14.351065593225039, Validation Loss = 7.599856276424482\n",
      "Iteration 57: Training Loss = 14.250079910336774, Validation Loss = 7.536372761129795\n",
      "Iteration 58: Training Loss = 14.154829166484983, Validation Loss = 7.475306818581589\n",
      "Iteration 59: Training Loss = 14.057413809288757, Validation Loss = 7.413647449658326\n",
      "Iteration 60: Training Loss = 13.96372389226798, Validation Loss = 7.3495263125994175\n",
      "Iteration 61: Training Loss = 13.871572895395438, Validation Loss = 7.290845516949779\n",
      "Iteration 62: Training Loss = 13.7788083997206, Validation Loss = 7.231710472381074\n",
      "Iteration 63: Training Loss = 13.690319774603463, Validation Loss = 7.175882717731806\n",
      "Iteration 64: Training Loss = 13.602698698736551, Validation Loss = 7.120391171135839\n",
      "Iteration 65: Training Loss = 13.514773270772933, Validation Loss = 7.063654740568332\n",
      "Iteration 66: Training Loss = 13.430422392971735, Validation Loss = 7.008303660024633\n",
      "Iteration 67: Training Loss = 13.346978131608358, Validation Loss = 6.954155028628294\n",
      "Iteration 68: Training Loss = 13.265347181914322, Validation Loss = 6.902605516693491\n",
      "Iteration 69: Training Loss = 13.183335058888492, Validation Loss = 6.84831442798507\n",
      "Iteration 70: Training Loss = 13.104406165100562, Validation Loss = 6.799753400867335\n",
      "Iteration 71: Training Loss = 13.025566556227336, Validation Loss = 6.751322589759575\n",
      "Iteration 72: Training Loss = 12.949264421662686, Validation Loss = 6.7006983072992625\n",
      "Iteration 73: Training Loss = 12.872752857896842, Validation Loss = 6.652584558908486\n",
      "Iteration 74: Training Loss = 12.798966868300436, Validation Loss = 6.604697621595422\n",
      "Iteration 75: Training Loss = 12.726837972560626, Validation Loss = 6.559254906689263\n",
      "Iteration 76: Training Loss = 12.654439013374748, Validation Loss = 6.512321073942045\n",
      "Iteration 77: Training Loss = 12.584721479711714, Validation Loss = 6.467482126967407\n",
      "Iteration 78: Training Loss = 12.514606574775703, Validation Loss = 6.423114217795313\n",
      "Iteration 79: Training Loss = 12.447066845071683, Validation Loss = 6.379587681698371\n",
      "Iteration 80: Training Loss = 12.380715271626135, Validation Loss = 6.33785958746535\n",
      "Iteration 81: Training Loss = 12.314261464858195, Validation Loss = 6.2942026574500956\n",
      "Iteration 82: Training Loss = 12.250173836761851, Validation Loss = 6.252951894029093\n",
      "Iteration 83: Training Loss = 12.187126468789966, Validation Loss = 6.213935485807977\n",
      "Iteration 84: Training Loss = 12.123932559436366, Validation Loss = 6.171851009095865\n",
      "Iteration 85: Training Loss = 12.063029890285799, Validation Loss = 6.132491893930765\n",
      "Iteration 86: Training Loss = 12.002109160524201, Validation Loss = 6.09467161368882\n",
      "Iteration 87: Training Loss = 11.943173414487665, Validation Loss = 6.0571154281473625\n",
      "Iteration 88: Training Loss = 11.885377948440548, Validation Loss = 6.02148787440318\n",
      "Iteration 89: Training Loss = 11.827499884966974, Validation Loss = 5.986235292900587\n",
      "Iteration 90: Training Loss = 11.771785495656182, Validation Loss = 5.952117762343336\n",
      "Iteration 91: Training Loss = 11.71688887255138, Validation Loss = 5.9158748733309965\n",
      "Iteration 92: Training Loss = 11.661604969967037, Validation Loss = 5.8814755407654085\n",
      "Iteration 93: Training Loss = 11.608602679681395, Validation Loss = 5.84737996035145\n",
      "Iteration 94: Training Loss = 11.556262754408426, Validation Loss = 5.8143389748585115\n",
      "Iteration 95: Training Loss = 11.5039636497851, Validation Loss = 5.779921043656866\n",
      "Iteration 96: Training Loss = 11.453496567031992, Validation Loss = 5.74890882074658\n",
      "Iteration 97: Training Loss = 11.404070078721528, Validation Loss = 5.718846640966293\n",
      "Iteration 98: Training Loss = 11.354182592236057, Validation Loss = 5.687903941438729\n",
      "Iteration 99: Training Loss = 11.30639440459183, Validation Loss = 5.657455104305928\n",
      "Iteration 100: Training Loss = 11.259280360112765, Validation Loss = 5.628447578961086\n",
      "Iteration 101: Training Loss = 11.211853786661674, Validation Loss = 5.5999748881233105\n",
      "Iteration 102: Training Loss = 11.1664118918416, Validation Loss = 5.571216794577483\n",
      "Iteration 103: Training Loss = 11.12074470098937, Validation Loss = 5.54161079769916\n",
      "Iteration 104: Training Loss = 11.076625998643186, Validation Loss = 5.513109876830611\n",
      "Iteration 105: Training Loss = 11.033311979975686, Validation Loss = 5.48637626325851\n",
      "Iteration 106: Training Loss = 10.989853035945632, Validation Loss = 5.457995863937595\n",
      "Iteration 107: Training Loss = 10.947937056629426, Validation Loss = 5.431905803749871\n",
      "Iteration 108: Training Loss = 10.906833727036089, Validation Loss = 5.4061621166201865\n",
      "Iteration 109: Training Loss = 10.866436198488685, Validation Loss = 5.381050042402243\n",
      "Iteration 110: Training Loss = 10.82564712814625, Validation Loss = 5.356091003066545\n",
      "Iteration 111: Training Loss = 10.786329006964833, Validation Loss = 5.330372978512853\n",
      "Iteration 112: Training Loss = 10.747898758918955, Validation Loss = 5.306896641978196\n",
      "Iteration 113: Training Loss = 10.709243166059085, Validation Loss = 5.283274620357943\n",
      "Iteration 114: Training Loss = 10.671896548605678, Validation Loss = 5.261226448299912\n",
      "Iteration 115: Training Loss = 10.63438753090811, Validation Loss = 5.238851462733958\n",
      "Iteration 116: Training Loss = 10.598329639590185, Validation Loss = 5.216345943924266\n",
      "Iteration 117: Training Loss = 10.562877605446216, Validation Loss = 5.193440319001225\n",
      "Iteration 118: Training Loss = 10.528075176654148, Validation Loss = 5.171057865612257\n",
      "Iteration 119: Training Loss = 10.493548500232425, Validation Loss = 5.151329950481793\n",
      "Iteration 120: Training Loss = 10.459010781428137, Validation Loss = 5.1310543880097725\n",
      "Iteration 121: Training Loss = 10.425944433281526, Validation Loss = 5.111721650479434\n",
      "Iteration 122: Training Loss = 10.39253335133468, Validation Loss = 5.092957243638202\n",
      "Iteration 123: Training Loss = 10.360551464788657, Validation Loss = 5.073143116511317\n",
      "Iteration 124: Training Loss = 10.329034480249376, Validation Loss = 5.052639608743681\n",
      "Iteration 125: Training Loss = 10.297275563039983, Validation Loss = 5.033986542556079\n",
      "Iteration 126: Training Loss = 10.266903133896282, Validation Loss = 5.01499747165813\n",
      "Iteration 127: Training Loss = 10.236897571319524, Validation Loss = 4.997084361927009\n",
      "Iteration 128: Training Loss = 10.20702701122967, Validation Loss = 4.97897970519916\n",
      "Iteration 129: Training Loss = 10.178049413434087, Validation Loss = 4.961843088715718\n",
      "Iteration 130: Training Loss = 10.148267038740709, Validation Loss = 4.943247635900386\n",
      "Iteration 131: Training Loss = 10.120339195899668, Validation Loss = 4.926894858988466\n",
      "Iteration 132: Training Loss = 10.092639434946983, Validation Loss = 4.91040775533154\n",
      "Iteration 133: Training Loss = 10.064307978096474, Validation Loss = 4.892756757721867\n",
      "Iteration 134: Training Loss = 10.03719762725293, Validation Loss = 4.877646098045864\n",
      "Iteration 135: Training Loss = 10.010984495806882, Validation Loss = 4.862520500199205\n",
      "Iteration 136: Training Loss = 9.984060116750385, Validation Loss = 4.845619845099284\n",
      "Iteration 137: Training Loss = 9.95860275602353, Validation Loss = 4.829351818364812\n",
      "Iteration 138: Training Loss = 9.933469165814495, Validation Loss = 4.813827388154339\n",
      "Iteration 139: Training Loss = 9.907840581369634, Validation Loss = 4.798002741745165\n",
      "Iteration 140: Training Loss = 9.883788004687364, Validation Loss = 4.783712467053177\n",
      "Iteration 141: Training Loss = 9.859733929749314, Validation Loss = 4.769759258630901\n",
      "Iteration 142: Training Loss = 9.836247395278576, Validation Loss = 4.755877403605373\n",
      "Iteration 143: Training Loss = 9.812217751369824, Validation Loss = 4.741003059139951\n",
      "Iteration 144: Training Loss = 9.78940299662897, Validation Loss = 4.7276668670841415\n",
      "Iteration 145: Training Loss = 9.767118533688256, Validation Loss = 4.715145409464267\n",
      "Iteration 146: Training Loss = 9.745067016616344, Validation Loss = 4.702431965760973\n",
      "Iteration 147: Training Loss = 9.72253265467174, Validation Loss = 4.688625630528976\n",
      "Iteration 148: Training Loss = 9.701341109732407, Validation Loss = 4.675517594686695\n",
      "Iteration 149: Training Loss = 9.68056339499312, Validation Loss = 4.663122295748227\n",
      "Iteration 150: Training Loss = 9.659130153439978, Validation Loss = 4.650120727713317\n",
      "Mean Squared Error on Validation Set: 4.650120727713317\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1          17.5113           23.68m\n",
      "         2          17.3230           23.28m\n",
      "         3          17.1384           22.98m\n",
      "         4          16.9571           22.80m\n",
      "         5          16.7792           22.76m\n",
      "         6          16.6042           22.65m\n",
      "         7          16.4329           22.49m\n",
      "         8          16.2645           22.35m\n",
      "         9          16.0996           22.15m\n",
      "        10          15.9360           21.99m\n",
      "        20          14.4576           20.46m\n",
      "        30          13.2172           19.09m\n",
      "        40          12.1773           21.50m\n",
      "        50          11.2989           19.17m\n",
      "        60          10.5600           17.01m\n",
      "        70           9.9371           14.74m\n",
      "        80           9.4049           12.64m\n",
      "        90           8.9563           10.68m\n",
      "       100           8.5718            8.78m\n",
      "Model for store TX_3 saved as model_store_TX_3_GB_0218.joblib\n",
      "Iteration 1: Training Loss = 17.511256836546696, Validation Loss = 12.944082701153844\n",
      "Iteration 2: Training Loss = 17.323003864271538, Validation Loss = 12.804371818155454\n",
      "Iteration 3: Training Loss = 17.138359044306394, Validation Loss = 12.675943627767174\n",
      "Iteration 4: Training Loss = 16.957051654271112, Validation Loss = 12.54127284934596\n",
      "Iteration 5: Training Loss = 16.779217812110968, Validation Loss = 12.417555816050346\n",
      "Iteration 6: Training Loss = 16.60415426909651, Validation Loss = 12.290757776566009\n",
      "Iteration 7: Training Loss = 16.432921704125356, Validation Loss = 12.172573189805604\n",
      "Iteration 8: Training Loss = 16.264511214361125, Validation Loss = 12.046896586746392\n",
      "Iteration 9: Training Loss = 16.09958468528772, Validation Loss = 11.932151498868727\n",
      "Iteration 10: Training Loss = 15.93600458023113, Validation Loss = 11.815724268590447\n",
      "Iteration 11: Training Loss = 15.776844185421606, Validation Loss = 11.705676426643775\n",
      "Iteration 12: Training Loss = 15.62012315230134, Validation Loss = 11.590000837491136\n",
      "Iteration 13: Training Loss = 15.46696519480828, Validation Loss = 11.482780625240672\n",
      "Iteration 14: Training Loss = 15.31439693140881, Validation Loss = 11.371557607593523\n",
      "Iteration 15: Training Loss = 15.164677064189556, Validation Loss = 11.261643584559376\n",
      "Iteration 16: Training Loss = 15.017667415646054, Validation Loss = 11.153226294447919\n",
      "Iteration 17: Training Loss = 14.873634090619358, Validation Loss = 11.047371141869762\n",
      "Iteration 18: Training Loss = 14.731420454025786, Validation Loss = 10.942493453770796\n",
      "Iteration 19: Training Loss = 14.593246426183587, Validation Loss = 10.839413986367529\n",
      "Iteration 20: Training Loss = 14.457623909465985, Validation Loss = 10.73874168827764\n",
      "Iteration 21: Training Loss = 14.323055900524098, Validation Loss = 10.639024420677874\n",
      "Iteration 22: Training Loss = 14.19092786842763, Validation Loss = 10.539250906936132\n",
      "Iteration 23: Training Loss = 14.06132538164597, Validation Loss = 10.443024866569793\n",
      "Iteration 24: Training Loss = 13.934522376195828, Validation Loss = 10.349089739486521\n",
      "Iteration 25: Training Loss = 13.809674915161526, Validation Loss = 10.258160589674285\n",
      "Iteration 26: Training Loss = 13.686935733609152, Validation Loss = 10.166585750912898\n",
      "Iteration 27: Training Loss = 13.566529461604526, Validation Loss = 10.078178494602122\n",
      "Iteration 28: Training Loss = 13.448311998458493, Validation Loss = 9.989151697685465\n",
      "Iteration 29: Training Loss = 13.331781271184296, Validation Loss = 9.902587301023717\n",
      "Iteration 30: Training Loss = 13.217200346508228, Validation Loss = 9.81850399497678\n",
      "Iteration 31: Training Loss = 13.105556256598723, Validation Loss = 9.7367827990359\n",
      "Iteration 32: Training Loss = 12.994939378892695, Validation Loss = 9.654729595356171\n",
      "Iteration 33: Training Loss = 12.886830791585671, Validation Loss = 9.573819787328526\n",
      "Iteration 34: Training Loss = 12.780104470858525, Validation Loss = 9.495458798369672\n",
      "Iteration 35: Training Loss = 12.675734419605684, Validation Loss = 9.42134918074585\n",
      "Iteration 36: Training Loss = 12.571457161561462, Validation Loss = 9.344145883345558\n",
      "Iteration 37: Training Loss = 12.469993488594215, Validation Loss = 9.270697675477845\n",
      "Iteration 38: Training Loss = 12.370844600759893, Validation Loss = 9.196456851582097\n",
      "Iteration 39: Training Loss = 12.27320624947834, Validation Loss = 9.123471102727338\n",
      "Iteration 40: Training Loss = 12.177281400030168, Validation Loss = 9.051139277802664\n",
      "Iteration 41: Training Loss = 12.08175605274699, Validation Loss = 8.980179944918603\n",
      "Iteration 42: Training Loss = 11.988486433190142, Validation Loss = 8.912566102597898\n",
      "Iteration 43: Training Loss = 11.89733335954133, Validation Loss = 8.84431964810145\n",
      "Iteration 44: Training Loss = 11.806760252245143, Validation Loss = 8.777159735538778\n",
      "Iteration 45: Training Loss = 11.717968328844218, Validation Loss = 8.711283599179046\n",
      "Iteration 46: Training Loss = 11.631609307156623, Validation Loss = 8.647568276006213\n",
      "Iteration 47: Training Loss = 11.54615467551881, Validation Loss = 8.585092618285957\n",
      "Iteration 48: Training Loss = 11.462000845329912, Validation Loss = 8.522157226937807\n",
      "Iteration 49: Training Loss = 11.379553296741882, Validation Loss = 8.462187802986557\n",
      "Iteration 50: Training Loss = 11.298880049794024, Validation Loss = 8.401755734508532\n",
      "Iteration 51: Training Loss = 11.218986929795221, Validation Loss = 8.342267050739206\n",
      "Iteration 52: Training Loss = 11.140571827879908, Validation Loss = 8.282654441199584\n",
      "Iteration 53: Training Loss = 11.063374383114782, Validation Loss = 8.22420329265955\n",
      "Iteration 54: Training Loss = 10.987545604843739, Validation Loss = 8.168048968328241\n",
      "Iteration 55: Training Loss = 10.91341635548031, Validation Loss = 8.113687420551768\n",
      "Iteration 56: Training Loss = 10.840076255943936, Validation Loss = 8.060270074206572\n",
      "Iteration 57: Training Loss = 10.768222547359192, Validation Loss = 8.005414978205918\n",
      "Iteration 58: Training Loss = 10.697906608718391, Validation Loss = 7.953978608430185\n",
      "Iteration 59: Training Loss = 10.628484039834078, Validation Loss = 7.901828564209729\n",
      "Iteration 60: Training Loss = 10.560020396637807, Validation Loss = 7.850430989140111\n",
      "Iteration 61: Training Loss = 10.493120610487754, Validation Loss = 7.799421582148638\n",
      "Iteration 62: Training Loss = 10.4267939372696, Validation Loss = 7.750164719326618\n",
      "Iteration 63: Training Loss = 10.361911275730384, Validation Loss = 7.704667157589354\n",
      "Iteration 64: Training Loss = 10.298135184424211, Validation Loss = 7.6564679613915105\n",
      "Iteration 65: Training Loss = 10.235414158731592, Validation Loss = 7.6088941500617056\n",
      "Iteration 66: Training Loss = 10.173820498107126, Validation Loss = 7.5648818079123945\n",
      "Iteration 67: Training Loss = 10.113135880737817, Validation Loss = 7.5187793694337275\n",
      "Iteration 68: Training Loss = 10.053430485833113, Validation Loss = 7.473345565800881\n",
      "Iteration 69: Training Loss = 9.994716278967891, Validation Loss = 7.431516958918057\n",
      "Iteration 70: Training Loss = 9.937092981895896, Validation Loss = 7.387487894506532\n",
      "Iteration 71: Training Loss = 9.880420801593335, Validation Loss = 7.34440157297036\n",
      "Iteration 72: Training Loss = 9.822301621758083, Validation Loss = 7.298710187593729\n",
      "Iteration 73: Training Loss = 9.767446091262682, Validation Loss = 7.256085090394896\n",
      "Iteration 74: Training Loss = 9.713732816021452, Validation Loss = 7.214807768708726\n",
      "Iteration 75: Training Loss = 9.660230008528375, Validation Loss = 7.174979173026078\n",
      "Iteration 76: Training Loss = 9.60803830597689, Validation Loss = 7.134809558527337\n",
      "Iteration 77: Training Loss = 9.556575391193673, Validation Loss = 7.09651066302746\n",
      "Iteration 78: Training Loss = 9.5058472033973, Validation Loss = 7.05900757081638\n",
      "Iteration 79: Training Loss = 9.453953332778575, Validation Loss = 7.018288144556738\n",
      "Iteration 80: Training Loss = 9.404865545505793, Validation Loss = 6.981700895943218\n",
      "Iteration 81: Training Loss = 9.357084982741767, Validation Loss = 6.94492878940509\n",
      "Iteration 82: Training Loss = 9.310094117875028, Validation Loss = 6.908749076488755\n",
      "Iteration 83: Training Loss = 9.26378295989799, Validation Loss = 6.872509415207216\n",
      "Iteration 84: Training Loss = 9.218288572112577, Validation Loss = 6.837648720559017\n",
      "Iteration 85: Training Loss = 9.173283880455958, Validation Loss = 6.804528904777329\n",
      "Iteration 86: Training Loss = 9.128978672824111, Validation Loss = 6.771736120832019\n",
      "Iteration 87: Training Loss = 9.084619816065926, Validation Loss = 6.73668554582537\n",
      "Iteration 88: Training Loss = 9.04171999903314, Validation Loss = 6.705238686368926\n",
      "Iteration 89: Training Loss = 8.9997296875702, Validation Loss = 6.675391449637077\n",
      "Iteration 90: Training Loss = 8.9562565518245, Validation Loss = 6.641192853281403\n",
      "Iteration 91: Training Loss = 8.915423993788849, Validation Loss = 6.609254571138207\n",
      "Iteration 92: Training Loss = 8.87345246064641, Validation Loss = 6.5759087136333365\n",
      "Iteration 93: Training Loss = 8.834070714710922, Validation Loss = 6.547000657194745\n",
      "Iteration 94: Training Loss = 8.795254705642845, Validation Loss = 6.518584461663684\n",
      "Iteration 95: Training Loss = 8.756990467872196, Validation Loss = 6.49174274133408\n",
      "Iteration 96: Training Loss = 8.717684264426826, Validation Loss = 6.462848866455031\n",
      "Iteration 97: Training Loss = 8.680897802060569, Validation Loss = 6.434895551734885\n",
      "Iteration 98: Training Loss = 8.644365394156617, Validation Loss = 6.407414453089962\n",
      "Iteration 99: Training Loss = 8.608696269252997, Validation Loss = 6.38050447232216\n",
      "Iteration 100: Training Loss = 8.571771428158387, Validation Loss = 6.352795596884865\n",
      "Iteration 101: Training Loss = 8.537314494069308, Validation Loss = 6.3258824859637315\n",
      "Iteration 102: Training Loss = 8.503128410751803, Validation Loss = 6.3009264409484285\n",
      "Iteration 103: Training Loss = 8.467977460604287, Validation Loss = 6.274747115882183\n",
      "Iteration 104: Training Loss = 8.434976190302672, Validation Loss = 6.250925606405192\n",
      "Iteration 105: Training Loss = 8.402629646690281, Validation Loss = 6.227069782166922\n",
      "Iteration 106: Training Loss = 8.370708509870699, Validation Loss = 6.203023504771724\n",
      "Iteration 107: Training Loss = 8.337797494375808, Validation Loss = 6.178246515118802\n",
      "Iteration 108: Training Loss = 8.307047713582424, Validation Loss = 6.155022637741364\n",
      "Iteration 109: Training Loss = 8.276826895085634, Validation Loss = 6.133991889281287\n",
      "Iteration 110: Training Loss = 8.245527914482869, Validation Loss = 6.110948565183686\n",
      "Iteration 111: Training Loss = 8.216321034911529, Validation Loss = 6.087433123827236\n",
      "Iteration 112: Training Loss = 8.187346690585345, Validation Loss = 6.065726958588673\n",
      "Iteration 113: Training Loss = 8.157556823753994, Validation Loss = 6.043654898771966\n",
      "Iteration 114: Training Loss = 8.129623664379217, Validation Loss = 6.021456748821598\n",
      "Iteration 115: Training Loss = 8.102283355001118, Validation Loss = 6.000101057353483\n",
      "Iteration 116: Training Loss = 8.075233996025947, Validation Loss = 5.979463083551839\n",
      "Iteration 117: Training Loss = 8.04727588436128, Validation Loss = 5.957208873700762\n",
      "Iteration 118: Training Loss = 8.02109292805881, Validation Loss = 5.937578052671177\n",
      "Iteration 119: Training Loss = 7.995473054476666, Validation Loss = 5.917775641417792\n",
      "Iteration 120: Training Loss = 7.968851013163192, Validation Loss = 5.896761791365953\n",
      "Iteration 121: Training Loss = 7.942244031989914, Validation Loss = 5.875629243002596\n",
      "Iteration 122: Training Loss = 7.917771619019574, Validation Loss = 5.855828303080201\n",
      "Iteration 123: Training Loss = 7.892429971476718, Validation Loss = 5.835726114475357\n",
      "Iteration 124: Training Loss = 7.868701734612357, Validation Loss = 5.818204716559201\n",
      "Iteration 125: Training Loss = 7.845318313362244, Validation Loss = 5.799562564987464\n",
      "Iteration 126: Training Loss = 7.821181318663514, Validation Loss = 5.7807718408349675\n",
      "Iteration 127: Training Loss = 7.7985692718782955, Validation Loss = 5.764523433618255\n",
      "Iteration 128: Training Loss = 7.7763807946447265, Validation Loss = 5.7468656164492025\n",
      "Iteration 129: Training Loss = 7.752797828680764, Validation Loss = 5.728351659939875\n",
      "Iteration 130: Training Loss = 7.731251373363255, Validation Loss = 5.711441990488455\n",
      "Iteration 131: Training Loss = 7.709768950687729, Validation Loss = 5.6951542010342076\n",
      "Iteration 132: Training Loss = 7.6874137493844374, Validation Loss = 5.677111208609275\n",
      "Iteration 133: Training Loss = 7.666847334633056, Validation Loss = 5.662027163369902\n",
      "Iteration 134: Training Loss = 7.6466124370032835, Validation Loss = 5.646000700724055\n",
      "Iteration 135: Training Loss = 7.625183564304124, Validation Loss = 5.629068933521428\n",
      "Iteration 136: Training Loss = 7.605392973185447, Validation Loss = 5.613805932114355\n",
      "Iteration 137: Training Loss = 7.585917945863141, Validation Loss = 5.599323829502987\n",
      "Iteration 138: Training Loss = 7.565465002991708, Validation Loss = 5.583147332800862\n",
      "Iteration 139: Training Loss = 7.546837158444634, Validation Loss = 5.568245291387902\n",
      "Iteration 140: Training Loss = 7.52854514300206, Validation Loss = 5.5537581334096\n",
      "Iteration 141: Training Loss = 7.509249590799983, Validation Loss = 5.538090588993859\n",
      "Iteration 142: Training Loss = 7.491441021735509, Validation Loss = 5.5245465767042985\n",
      "Iteration 143: Training Loss = 7.473998694750054, Validation Loss = 5.510829336881889\n",
      "Iteration 144: Training Loss = 7.45537539753565, Validation Loss = 5.496092701593697\n",
      "Iteration 145: Training Loss = 7.4383685903250045, Validation Loss = 5.483025738258443\n",
      "Iteration 146: Training Loss = 7.42156041596258, Validation Loss = 5.469864334331518\n",
      "Iteration 147: Training Loss = 7.403934250358544, Validation Loss = 5.455510158396421\n",
      "Iteration 148: Training Loss = 7.38771504489617, Validation Loss = 5.442876692401595\n",
      "Iteration 149: Training Loss = 7.370919563242111, Validation Loss = 5.4293021051246555\n",
      "Iteration 150: Training Loss = 7.355161477254795, Validation Loss = 5.417072060150926\n",
      "Mean Squared Error on Validation Set: 5.417072060150926\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           7.2704           22.03m\n",
      "         2           7.2135           21.82m\n",
      "         3           7.1578           21.54m\n",
      "         4           7.1032           21.56m\n",
      "         5           7.0497           21.47m\n",
      "         6           6.9972           21.37m\n",
      "         7           6.9460           21.21m\n",
      "         8           6.8955           21.05m\n",
      "         9           6.8453           20.89m\n",
      "        10           6.7971           20.72m\n",
      "        20           6.3544           19.21m\n",
      "        30           5.9807           17.75m\n",
      "        40           5.6648           16.26m\n",
      "        50           5.3945           14.81m\n",
      "        60           5.1635           14.71m\n",
      "        70           4.9667           14.55m\n",
      "        80           4.7967           13.71m\n",
      "        90           4.6520           12.41m\n",
      "       100           4.5274           10.76m\n",
      "Model for store WI_1 saved as model_store_WI_1_GB_0218.joblib\n",
      "Iteration 1: Training Loss = 7.270352517182183, Validation Loss = 6.599276894389846\n",
      "Iteration 2: Training Loss = 7.2135215607821825, Validation Loss = 6.546872352482177\n",
      "Iteration 3: Training Loss = 7.157820720656194, Validation Loss = 6.495525975491004\n",
      "Iteration 4: Training Loss = 7.103225612339884, Validation Loss = 6.445135928913344\n",
      "Iteration 5: Training Loss = 7.049712025721053, Validation Loss = 6.395791472357257\n",
      "Iteration 6: Training Loss = 6.997153502444797, Validation Loss = 6.346846296294912\n",
      "Iteration 7: Training Loss = 6.946021709995619, Validation Loss = 6.2994579764440015\n",
      "Iteration 8: Training Loss = 6.89547253044602, Validation Loss = 6.252785697817759\n",
      "Iteration 9: Training Loss = 6.84529907913078, Validation Loss = 6.205785017606787\n",
      "Iteration 10: Training Loss = 6.797111448224496, Validation Loss = 6.161303800499231\n",
      "Iteration 11: Training Loss = 6.749859661480895, Validation Loss = 6.117287782409746\n",
      "Iteration 12: Training Loss = 6.702283192283404, Validation Loss = 6.072740963311898\n",
      "Iteration 13: Training Loss = 6.656671648729093, Validation Loss = 6.030313184808632\n",
      "Iteration 14: Training Loss = 6.610788138648592, Validation Loss = 5.987794919622069\n",
      "Iteration 15: Training Loss = 6.5668472068346695, Validation Loss = 5.94727213126457\n",
      "Iteration 16: Training Loss = 6.522589199802768, Validation Loss = 5.905933774795644\n",
      "Iteration 17: Training Loss = 6.479585674871918, Validation Loss = 5.865951638593944\n",
      "Iteration 18: Training Loss = 6.437441911316248, Validation Loss = 5.826923012987814\n",
      "Iteration 19: Training Loss = 6.395539812224441, Validation Loss = 5.787887581376636\n",
      "Iteration 20: Training Loss = 6.35435850580071, Validation Loss = 5.74948904466976\n",
      "Iteration 21: Training Loss = 6.313639311531613, Validation Loss = 5.711901146376386\n",
      "Iteration 22: Training Loss = 6.2742174148628465, Validation Loss = 5.675272343917382\n",
      "Iteration 23: Training Loss = 6.23512920849443, Validation Loss = 5.63942611916414\n",
      "Iteration 24: Training Loss = 6.196976096915389, Validation Loss = 5.60420682450493\n",
      "Iteration 25: Training Loss = 6.1594313582764775, Validation Loss = 5.569090886848548\n",
      "Iteration 26: Training Loss = 6.1224907381053555, Validation Loss = 5.534609119731996\n",
      "Iteration 27: Training Loss = 6.086373105450542, Validation Loss = 5.500606084678071\n",
      "Iteration 28: Training Loss = 6.0506069405286445, Validation Loss = 5.467388895597462\n",
      "Iteration 29: Training Loss = 6.01576871917936, Validation Loss = 5.435181716813439\n",
      "Iteration 30: Training Loss = 5.9807159688186475, Validation Loss = 5.403023013266438\n",
      "Iteration 31: Training Loss = 5.946824414616306, Validation Loss = 5.371146125657287\n",
      "Iteration 32: Training Loss = 5.912861355334028, Validation Loss = 5.340001843943366\n",
      "Iteration 33: Training Loss = 5.880330496706097, Validation Loss = 5.309244337892987\n",
      "Iteration 34: Training Loss = 5.84802559554923, Validation Loss = 5.279380156194867\n",
      "Iteration 35: Training Loss = 5.8162535130330095, Validation Loss = 5.249820189657159\n",
      "Iteration 36: Training Loss = 5.785142997878132, Validation Loss = 5.220726739036011\n",
      "Iteration 37: Training Loss = 5.753894206430179, Validation Loss = 5.191982824792365\n",
      "Iteration 38: Training Loss = 5.723684862916587, Validation Loss = 5.164453515781953\n",
      "Iteration 39: Training Loss = 5.694206686759548, Validation Loss = 5.13703931300872\n",
      "Iteration 40: Training Loss = 5.664765197543043, Validation Loss = 5.109616615138752\n",
      "Iteration 41: Training Loss = 5.636100015140206, Validation Loss = 5.082554784404333\n",
      "Iteration 42: Training Loss = 5.607322731470909, Validation Loss = 5.0562708688708256\n",
      "Iteration 43: Training Loss = 5.57942709200411, Validation Loss = 5.030598859960227\n",
      "Iteration 44: Training Loss = 5.5513230981614905, Validation Loss = 5.004661638760136\n",
      "Iteration 45: Training Loss = 5.523766030361619, Validation Loss = 4.979499202108934\n",
      "Iteration 46: Training Loss = 5.496735520263303, Validation Loss = 4.955023509829412\n",
      "Iteration 47: Training Loss = 5.470886361124906, Validation Loss = 4.931094823947644\n",
      "Iteration 48: Training Loss = 5.445143822071138, Validation Loss = 4.907320968525068\n",
      "Iteration 49: Training Loss = 5.419758388868367, Validation Loss = 4.883830780742042\n",
      "Iteration 50: Training Loss = 5.394471817719761, Validation Loss = 4.861269181597538\n",
      "Iteration 51: Training Loss = 5.369304190177715, Validation Loss = 4.8382918806914\n",
      "Iteration 52: Training Loss = 5.344874335276403, Validation Loss = 4.8159704303672735\n",
      "Iteration 53: Training Loss = 5.320668027319181, Validation Loss = 4.794079656353929\n",
      "Iteration 54: Training Loss = 5.29725102404216, Validation Loss = 4.772962073073531\n",
      "Iteration 55: Training Loss = 5.273684965404837, Validation Loss = 4.751500368775006\n",
      "Iteration 56: Training Loss = 5.2511127172091445, Validation Loss = 4.7303769369120845\n",
      "Iteration 57: Training Loss = 5.22892970265811, Validation Loss = 4.709585668230789\n",
      "Iteration 58: Training Loss = 5.207030410386152, Validation Loss = 4.689865526452287\n",
      "Iteration 59: Training Loss = 5.185118230204965, Validation Loss = 4.669961276157143\n",
      "Iteration 60: Training Loss = 5.163529440126686, Validation Loss = 4.650330277601688\n",
      "Iteration 61: Training Loss = 5.142704109288095, Validation Loss = 4.630612944033261\n",
      "Iteration 62: Training Loss = 5.1218490911241705, Validation Loss = 4.611702077411204\n",
      "Iteration 63: Training Loss = 5.101092062191951, Validation Loss = 4.592844846728812\n",
      "Iteration 64: Training Loss = 5.080903952916894, Validation Loss = 4.574128737879201\n",
      "Iteration 65: Training Loss = 5.060655186035998, Validation Loss = 4.555532056633417\n",
      "Iteration 66: Training Loss = 5.041341557956398, Validation Loss = 4.537959392203451\n",
      "Iteration 67: Training Loss = 5.0222716950023125, Validation Loss = 4.5204012097918405\n",
      "Iteration 68: Training Loss = 5.0032467110037855, Validation Loss = 4.502770538236536\n",
      "Iteration 69: Training Loss = 4.984841879038231, Validation Loss = 4.485308783898398\n",
      "Iteration 70: Training Loss = 4.966696475972843, Validation Loss = 4.468513859014\n",
      "Iteration 71: Training Loss = 4.948230371277808, Validation Loss = 4.451504661083786\n",
      "Iteration 72: Training Loss = 4.930316137736397, Validation Loss = 4.435174680626464\n",
      "Iteration 73: Training Loss = 4.912589092327341, Validation Loss = 4.418975371379421\n",
      "Iteration 74: Training Loss = 4.895312508958412, Validation Loss = 4.4032949870145375\n",
      "Iteration 75: Training Loss = 4.878328141775396, Validation Loss = 4.38738688520415\n",
      "Iteration 76: Training Loss = 4.861256305085057, Validation Loss = 4.3718356187401\n",
      "Iteration 77: Training Loss = 4.8449919282785965, Validation Loss = 4.356767335626698\n",
      "Iteration 78: Training Loss = 4.828745179932418, Validation Loss = 4.341756799788854\n",
      "Iteration 79: Training Loss = 4.81280624508085, Validation Loss = 4.327070350037457\n",
      "Iteration 80: Training Loss = 4.796724026644064, Validation Loss = 4.312494824327138\n",
      "Iteration 81: Training Loss = 4.781217404039902, Validation Loss = 4.298415071533675\n",
      "Iteration 82: Training Loss = 4.766045804209937, Validation Loss = 4.284451196692311\n",
      "Iteration 83: Training Loss = 4.750714537890459, Validation Loss = 4.270590956677099\n",
      "Iteration 84: Training Loss = 4.736114598978438, Validation Loss = 4.256891586934368\n",
      "Iteration 85: Training Loss = 4.721712568894541, Validation Loss = 4.2433294908349595\n",
      "Iteration 86: Training Loss = 4.707381055204678, Validation Loss = 4.230070059956086\n",
      "Iteration 87: Training Loss = 4.693365219777101, Validation Loss = 4.217305847291514\n",
      "Iteration 88: Training Loss = 4.679154664562391, Validation Loss = 4.204348464699809\n",
      "Iteration 89: Training Loss = 4.665408295829572, Validation Loss = 4.191754615693432\n",
      "Iteration 90: Training Loss = 4.652003304790844, Validation Loss = 4.179437838158431\n",
      "Iteration 91: Training Loss = 4.63852023683872, Validation Loss = 4.1671948317433065\n",
      "Iteration 92: Training Loss = 4.62561947452717, Validation Loss = 4.1554677973897896\n",
      "Iteration 93: Training Loss = 4.6127852126846, Validation Loss = 4.143800729070255\n",
      "Iteration 94: Training Loss = 4.599774502031482, Validation Loss = 4.132162451338378\n",
      "Iteration 95: Training Loss = 4.587106697579134, Validation Loss = 4.120091473740547\n",
      "Iteration 96: Training Loss = 4.574896715164164, Validation Loss = 4.109185998279028\n",
      "Iteration 97: Training Loss = 4.562687673155173, Validation Loss = 4.097983229840338\n",
      "Iteration 98: Training Loss = 4.55096229138997, Validation Loss = 4.086811184732706\n",
      "Iteration 99: Training Loss = 4.538907800627731, Validation Loss = 4.0758974113757995\n",
      "Iteration 100: Training Loss = 4.527408310995254, Validation Loss = 4.065041902084711\n",
      "Iteration 101: Training Loss = 4.515962461697411, Validation Loss = 4.054804707479059\n",
      "Iteration 102: Training Loss = 4.504789985175511, Validation Loss = 4.044461089077123\n",
      "Iteration 103: Training Loss = 4.493444626572909, Validation Loss = 4.03408351166213\n",
      "Iteration 104: Training Loss = 4.482503650306943, Validation Loss = 4.024214353824236\n",
      "Iteration 105: Training Loss = 4.4719089117273585, Validation Loss = 4.014334701631078\n",
      "Iteration 106: Training Loss = 4.461425535398198, Validation Loss = 4.00473007918324\n",
      "Iteration 107: Training Loss = 4.451037212979891, Validation Loss = 3.9954001808159894\n",
      "Iteration 108: Training Loss = 4.4407260956950605, Validation Loss = 3.985746349993877\n",
      "Iteration 109: Training Loss = 4.430626022336544, Validation Loss = 3.976642726753292\n",
      "Iteration 110: Training Loss = 4.420388838914405, Validation Loss = 3.9672934499292016\n",
      "Iteration 111: Training Loss = 4.410645859924401, Validation Loss = 3.9583297974054594\n",
      "Iteration 112: Training Loss = 4.4010356069934895, Validation Loss = 3.9494173618944295\n",
      "Iteration 113: Training Loss = 4.391666734666432, Validation Loss = 3.940695841425488\n",
      "Iteration 114: Training Loss = 4.382237731531646, Validation Loss = 3.932199767159407\n",
      "Iteration 115: Training Loss = 4.3727690613181815, Validation Loss = 3.9241389570251535\n",
      "Iteration 116: Training Loss = 4.363506017846725, Validation Loss = 3.9152450338726696\n",
      "Iteration 117: Training Loss = 4.354595425059344, Validation Loss = 3.9068494206128164\n",
      "Iteration 118: Training Loss = 4.345766264205671, Validation Loss = 3.8989367141965228\n",
      "Iteration 119: Training Loss = 4.337139774684824, Validation Loss = 3.890778966709603\n",
      "Iteration 120: Training Loss = 4.3283055834624635, Validation Loss = 3.882732696080244\n",
      "Iteration 121: Training Loss = 4.319878822274441, Validation Loss = 3.875148040418452\n",
      "Iteration 122: Training Loss = 4.311480790139359, Validation Loss = 3.8672659786996166\n",
      "Iteration 123: Training Loss = 4.30337879074802, Validation Loss = 3.8597563443230274\n",
      "Iteration 124: Training Loss = 4.295263200931165, Validation Loss = 3.8524035330418167\n",
      "Iteration 125: Training Loss = 4.287065956713118, Validation Loss = 3.8450810105932445\n",
      "Iteration 126: Training Loss = 4.279223099428167, Validation Loss = 3.837931647799558\n",
      "Iteration 127: Training Loss = 4.271508912120602, Validation Loss = 3.8310588368033653\n",
      "Iteration 128: Training Loss = 4.263904718658612, Validation Loss = 3.824027576606083\n",
      "Iteration 129: Training Loss = 4.2558746284457625, Validation Loss = 3.817372307252342\n",
      "Iteration 130: Training Loss = 4.2484777563990965, Validation Loss = 3.810601014469064\n",
      "Iteration 131: Training Loss = 4.2409539580627, Validation Loss = 3.8041776241672225\n",
      "Iteration 132: Training Loss = 4.233839380674118, Validation Loss = 3.7971148080606674\n",
      "Iteration 133: Training Loss = 4.226777769282419, Validation Loss = 3.7905342374576843\n",
      "Iteration 134: Training Loss = 4.21933499363671, Validation Loss = 3.7843495571257266\n",
      "Iteration 135: Training Loss = 4.212519715274458, Validation Loss = 3.778106793047509\n",
      "Iteration 136: Training Loss = 4.205756315109583, Validation Loss = 3.771886985286244\n",
      "Iteration 137: Training Loss = 4.199052942282648, Validation Loss = 3.7653056859267116\n",
      "Iteration 138: Training Loss = 4.192493026049087, Validation Loss = 3.759281039231582\n",
      "Iteration 139: Training Loss = 4.1857980073353325, Validation Loss = 3.7535701355008633\n",
      "Iteration 140: Training Loss = 4.179360344061862, Validation Loss = 3.7475699617065263\n",
      "Iteration 141: Training Loss = 4.173134407158134, Validation Loss = 3.7416444133025624\n",
      "Iteration 142: Training Loss = 4.166915419512604, Validation Loss = 3.7361427911288954\n",
      "Iteration 143: Training Loss = 4.160767313464798, Validation Loss = 3.730554877032953\n",
      "Iteration 144: Training Loss = 4.15432049561366, Validation Loss = 3.725282735888806\n",
      "Iteration 145: Training Loss = 4.148403578822587, Validation Loss = 3.7196558635153614\n",
      "Iteration 146: Training Loss = 4.142585654823422, Validation Loss = 3.7143001238158275\n",
      "Iteration 147: Training Loss = 4.136425303124695, Validation Loss = 3.7093522801686416\n",
      "Iteration 148: Training Loss = 4.130770246001083, Validation Loss = 3.703843998713383\n",
      "Iteration 149: Training Loss = 4.125211881908716, Validation Loss = 3.698804630232092\n",
      "Iteration 150: Training Loss = 4.119681978441831, Validation Loss = 3.6936364536692223\n",
      "Mean Squared Error on Validation Set: 3.6936364536692223\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1          18.6160           23.38m\n",
      "         2          18.4716           23.41m\n",
      "         3          18.3301           22.87m\n",
      "         4          18.1913           22.71m\n",
      "         5          18.0550           22.53m\n",
      "         6          17.9213           22.33m\n",
      "         7          17.7904           22.11m\n",
      "         8          17.6598           21.89m\n",
      "         9          17.5332           21.64m\n",
      "        10          17.4070           21.43m\n",
      "        20          16.2650           19.73m\n",
      "        30          15.3034           18.17m\n",
      "        40          14.4917           16.65m\n",
      "        50          13.8053           15.68m\n",
      "        60          13.2128           14.03m\n",
      "        70          12.7059           12.89m\n",
      "        80          12.2714           11.64m\n",
      "        90          11.9002           10.45m\n",
      "       100          11.5799            8.61m\n",
      "Model for store WI_2 saved as model_store_WI_2_GB_0218.joblib\n",
      "Iteration 1: Training Loss = 18.615980776320782, Validation Loss = 24.048006742945592\n",
      "Iteration 2: Training Loss = 18.47159612018507, Validation Loss = 23.843630541430088\n",
      "Iteration 3: Training Loss = 18.330068117601726, Validation Loss = 23.642796620015375\n",
      "Iteration 4: Training Loss = 18.191337757819266, Validation Loss = 23.445863850731627\n",
      "Iteration 5: Training Loss = 18.054975446405567, Validation Loss = 23.25189188511545\n",
      "Iteration 6: Training Loss = 17.921261068849592, Validation Loss = 23.0553996761062\n",
      "Iteration 7: Training Loss = 17.790364256597528, Validation Loss = 22.869744876470772\n",
      "Iteration 8: Training Loss = 17.659772469652978, Validation Loss = 22.69583549867665\n",
      "Iteration 9: Training Loss = 17.53318933750144, Validation Loss = 22.515020401374755\n",
      "Iteration 10: Training Loss = 17.40697165496904, Validation Loss = 22.347542838704555\n",
      "Iteration 11: Training Loss = 17.285277578624587, Validation Loss = 22.16925676939481\n",
      "Iteration 12: Training Loss = 17.1629633951695, Validation Loss = 21.994522415180942\n",
      "Iteration 13: Training Loss = 17.04441141106457, Validation Loss = 21.818768149213177\n",
      "Iteration 14: Training Loss = 16.925880153164094, Validation Loss = 21.648465897229876\n",
      "Iteration 15: Training Loss = 16.810361699498213, Validation Loss = 21.493064706616096\n",
      "Iteration 16: Training Loss = 16.698210259342396, Validation Loss = 21.329449275212053\n",
      "Iteration 17: Training Loss = 16.587768383942734, Validation Loss = 21.168341585843958\n",
      "Iteration 18: Training Loss = 16.47744945708399, Validation Loss = 21.010267998518657\n",
      "Iteration 19: Training Loss = 16.369285700275082, Validation Loss = 20.854565645633187\n",
      "Iteration 20: Training Loss = 16.264953322696133, Validation Loss = 20.697797617734466\n",
      "Iteration 21: Training Loss = 16.161783210673338, Validation Loss = 20.5406506440931\n",
      "Iteration 22: Training Loss = 16.058593989824363, Validation Loss = 20.390599649548772\n",
      "Iteration 23: Training Loss = 15.959208686606486, Validation Loss = 20.244976801510134\n",
      "Iteration 24: Training Loss = 15.859551231734441, Validation Loss = 20.098511617773926\n",
      "Iteration 25: Training Loss = 15.76357866948253, Validation Loss = 19.9515713014169\n",
      "Iteration 26: Training Loss = 15.668463116489491, Validation Loss = 19.815648947143135\n",
      "Iteration 27: Training Loss = 15.57369259341231, Validation Loss = 19.674011021831117\n",
      "Iteration 28: Training Loss = 15.482329781773771, Validation Loss = 19.543809485869474\n",
      "Iteration 29: Training Loss = 15.391957091489214, Validation Loss = 19.413936759526397\n",
      "Iteration 30: Training Loss = 15.303439800693, Validation Loss = 19.28662749028364\n",
      "Iteration 31: Training Loss = 15.214605886397155, Validation Loss = 19.153269332306017\n",
      "Iteration 32: Training Loss = 15.128896788124067, Validation Loss = 19.027127436402658\n",
      "Iteration 33: Training Loss = 15.04473672810691, Validation Loss = 18.90529135658653\n",
      "Iteration 34: Training Loss = 14.961874850511228, Validation Loss = 18.784406746660505\n",
      "Iteration 35: Training Loss = 14.88058460272763, Validation Loss = 18.66395779419543\n",
      "Iteration 36: Training Loss = 14.798717755757771, Validation Loss = 18.539421488214995\n",
      "Iteration 37: Training Loss = 14.719953997395532, Validation Loss = 18.422339971531887\n",
      "Iteration 38: Training Loss = 14.642614570650995, Validation Loss = 18.308809317587816\n",
      "Iteration 39: Training Loss = 14.566472100606298, Validation Loss = 18.19580993986349\n",
      "Iteration 40: Training Loss = 14.491680126510715, Validation Loss = 18.08389388312956\n",
      "Iteration 41: Training Loss = 14.4182562493115, Validation Loss = 17.976257990816574\n",
      "Iteration 42: Training Loss = 14.343848946720406, Validation Loss = 17.863266234836555\n",
      "Iteration 43: Training Loss = 14.272811574757132, Validation Loss = 17.755581667040428\n",
      "Iteration 44: Training Loss = 14.2027167926141, Validation Loss = 17.65064634260784\n",
      "Iteration 45: Training Loss = 14.133860137821864, Validation Loss = 17.548927258341553\n",
      "Iteration 46: Training Loss = 14.066063989323398, Validation Loss = 17.445571342729558\n",
      "Iteration 47: Training Loss = 13.999338782302745, Validation Loss = 17.34394063092881\n",
      "Iteration 48: Training Loss = 13.933906868418665, Validation Loss = 17.243278695895405\n",
      "Iteration 49: Training Loss = 13.868966808612148, Validation Loss = 17.14083932385653\n",
      "Iteration 50: Training Loss = 13.805338413339891, Validation Loss = 17.04215720352956\n",
      "Iteration 51: Training Loss = 13.740855620363687, Validation Loss = 16.953983787383553\n",
      "Iteration 52: Training Loss = 13.679022923699023, Validation Loss = 16.857537040860144\n",
      "Iteration 53: Training Loss = 13.618513928241697, Validation Loss = 16.763388152825986\n",
      "Iteration 54: Training Loss = 13.556772982561275, Validation Loss = 16.66822178827139\n",
      "Iteration 55: Training Loss = 13.498030008198361, Validation Loss = 16.57392574837325\n",
      "Iteration 56: Training Loss = 13.437986516555528, Validation Loss = 16.493214736061812\n",
      "Iteration 57: Training Loss = 13.380716281077547, Validation Loss = 16.404379861101674\n",
      "Iteration 58: Training Loss = 13.322280444739906, Validation Loss = 16.32643001407419\n",
      "Iteration 59: Training Loss = 13.267227969534463, Validation Loss = 16.239778140153433\n",
      "Iteration 60: Training Loss = 13.212846870209034, Validation Loss = 16.153224875289858\n",
      "Iteration 61: Training Loss = 13.157441262230801, Validation Loss = 16.079552953826767\n",
      "Iteration 62: Training Loss = 13.104631587472408, Validation Loss = 15.993262897306534\n",
      "Iteration 63: Training Loss = 13.051515189593582, Validation Loss = 15.920640002407053\n",
      "Iteration 64: Training Loss = 13.00054489957839, Validation Loss = 15.837396414175346\n",
      "Iteration 65: Training Loss = 12.949946752029645, Validation Loss = 15.755470963846738\n",
      "Iteration 66: Training Loss = 12.898289208196786, Validation Loss = 15.686710327870745\n",
      "Iteration 67: Training Loss = 12.849467155723744, Validation Loss = 15.609870693041033\n",
      "Iteration 68: Training Loss = 12.799759569290037, Validation Loss = 15.54383681445836\n",
      "Iteration 69: Training Loss = 12.752397857996625, Validation Loss = 15.469258672162702\n",
      "Iteration 70: Training Loss = 12.70590027255078, Validation Loss = 15.400989022185813\n",
      "Iteration 71: Training Loss = 12.658933845219407, Validation Loss = 15.336829518100167\n",
      "Iteration 72: Training Loss = 12.613970069974622, Validation Loss = 15.2633646777312\n",
      "Iteration 73: Training Loss = 12.567896019155588, Validation Loss = 15.20082854586336\n",
      "Iteration 74: Training Loss = 12.524109396509735, Validation Loss = 15.128598511820705\n",
      "Iteration 75: Training Loss = 12.480722582603535, Validation Loss = 15.061817529670629\n",
      "Iteration 76: Training Loss = 12.43691970200388, Validation Loss = 15.00295822007372\n",
      "Iteration 77: Training Loss = 12.395110084464045, Validation Loss = 14.934921509091513\n",
      "Iteration 78: Training Loss = 12.35309201883728, Validation Loss = 14.877432848092914\n",
      "Iteration 79: Training Loss = 12.31270439421944, Validation Loss = 14.809860301445124\n",
      "Iteration 80: Training Loss = 12.27139643642166, Validation Loss = 14.754308758219205\n",
      "Iteration 81: Training Loss = 12.232115515545376, Validation Loss = 14.68971181403164\n",
      "Iteration 82: Training Loss = 12.193034397746803, Validation Loss = 14.628240321174665\n",
      "Iteration 83: Training Loss = 12.154994377281863, Validation Loss = 14.57212510369176\n",
      "Iteration 84: Training Loss = 12.115927105510519, Validation Loss = 14.518214816694645\n",
      "Iteration 85: Training Loss = 12.079017313993667, Validation Loss = 14.461211633581236\n",
      "Iteration 86: Training Loss = 12.04160316011785, Validation Loss = 14.411221288895387\n",
      "Iteration 87: Training Loss = 12.005263204883471, Validation Loss = 14.350654714697278\n",
      "Iteration 88: Training Loss = 11.969649492236124, Validation Loss = 14.293287824229179\n",
      "Iteration 89: Training Loss = 11.934276173980544, Validation Loss = 14.24420496172099\n",
      "Iteration 90: Training Loss = 11.900210701485662, Validation Loss = 14.18642835842396\n",
      "Iteration 91: Training Loss = 11.86599423902411, Validation Loss = 14.129774750346126\n",
      "Iteration 92: Training Loss = 11.831611566417486, Validation Loss = 14.08286158117618\n",
      "Iteration 93: Training Loss = 11.798405541161964, Validation Loss = 14.029915016422603\n",
      "Iteration 94: Training Loss = 11.766263478553192, Validation Loss = 13.981474445844261\n",
      "Iteration 95: Training Loss = 11.733559103623836, Validation Loss = 13.936500439755129\n",
      "Iteration 96: Training Loss = 11.702256878392383, Validation Loss = 13.88472184603796\n",
      "Iteration 97: Training Loss = 11.67115562306119, Validation Loss = 13.834108856681096\n",
      "Iteration 98: Training Loss = 11.640077959202902, Validation Loss = 13.792106353404991\n",
      "Iteration 99: Training Loss = 11.610518445733819, Validation Loss = 13.741952377704342\n",
      "Iteration 100: Training Loss = 11.579914435266435, Validation Loss = 13.697981129542933\n",
      "Iteration 101: Training Loss = 11.550832867854812, Validation Loss = 13.649499720128317\n",
      "Iteration 102: Training Loss = 11.52182489971274, Validation Loss = 13.602688854523599\n",
      "Iteration 103: Training Loss = 11.492733963750691, Validation Loss = 13.56249875567315\n",
      "Iteration 104: Training Loss = 11.464704935113092, Validation Loss = 13.516893430392866\n",
      "Iteration 105: Training Loss = 11.437204054997077, Validation Loss = 13.47372565284374\n",
      "Iteration 106: Training Loss = 11.40951262711522, Validation Loss = 13.433701391339234\n",
      "Iteration 107: Training Loss = 11.383238387042944, Validation Loss = 13.393512894732984\n",
      "Iteration 108: Training Loss = 11.356609971014388, Validation Loss = 13.356738775730294\n",
      "Iteration 109: Training Loss = 11.330832326072267, Validation Loss = 13.315343040375318\n",
      "Iteration 110: Training Loss = 11.305519936486343, Validation Loss = 13.271095785827017\n",
      "Iteration 111: Training Loss = 11.28052880579658, Validation Loss = 13.229150244715369\n",
      "Iteration 112: Training Loss = 11.255221856165251, Validation Loss = 13.1920217388092\n",
      "Iteration 113: Training Loss = 11.230648593764966, Validation Loss = 13.151411320321523\n",
      "Iteration 114: Training Loss = 11.206329866197626, Validation Loss = 13.117329976234084\n",
      "Iteration 115: Training Loss = 11.18289969003289, Validation Loss = 13.079181522348543\n",
      "Iteration 116: Training Loss = 11.159788581804008, Validation Loss = 13.043302038065029\n",
      "Iteration 117: Training Loss = 11.136488061999383, Validation Loss = 13.004991546971535\n",
      "Iteration 118: Training Loss = 11.114142212310819, Validation Loss = 12.965601152772459\n",
      "Iteration 119: Training Loss = 11.09220900895709, Validation Loss = 12.929724777042294\n",
      "Iteration 120: Training Loss = 11.070252074573036, Validation Loss = 12.897131967186342\n",
      "Iteration 121: Training Loss = 11.0480470137877, Validation Loss = 12.864578261275161\n",
      "Iteration 122: Training Loss = 11.0268679622233, Validation Loss = 12.827973995014574\n",
      "Iteration 123: Training Loss = 11.005361719656769, Validation Loss = 12.79753323454256\n",
      "Iteration 124: Training Loss = 10.984614291443458, Validation Loss = 12.763493019427846\n",
      "Iteration 125: Training Loss = 10.964034603496547, Validation Loss = 12.729047337387765\n",
      "Iteration 126: Training Loss = 10.943964162890023, Validation Loss = 12.694147318841036\n",
      "Iteration 127: Training Loss = 10.92373478225902, Validation Loss = 12.66524526513603\n",
      "Iteration 128: Training Loss = 10.904580997493449, Validation Loss = 12.631580967872837\n",
      "Iteration 129: Training Loss = 10.8851269767291, Validation Loss = 12.604165587500441\n",
      "Iteration 130: Training Loss = 10.866409599963427, Validation Loss = 12.571514038645695\n",
      "Iteration 131: Training Loss = 10.847963351524669, Validation Loss = 12.541478233381387\n",
      "Iteration 132: Training Loss = 10.829584616722903, Validation Loss = 12.510844574871419\n",
      "Iteration 133: Training Loss = 10.811533184599398, Validation Loss = 12.480464695604795\n",
      "Iteration 134: Training Loss = 10.794056459113001, Validation Loss = 12.453048224816456\n",
      "Iteration 135: Training Loss = 10.776071625295767, Validation Loss = 12.427035868912125\n",
      "Iteration 136: Training Loss = 10.7591089855424, Validation Loss = 12.398202889920688\n",
      "Iteration 137: Training Loss = 10.742314304257466, Validation Loss = 12.371209967050005\n",
      "Iteration 138: Training Loss = 10.725288033815966, Validation Loss = 12.342089138267175\n",
      "Iteration 139: Training Loss = 10.708221255939574, Validation Loss = 12.317254094037331\n",
      "Iteration 140: Training Loss = 10.691893863413386, Validation Loss = 12.289526802097875\n",
      "Iteration 141: Training Loss = 10.676004814791256, Validation Loss = 12.262942076530589\n",
      "Iteration 142: Training Loss = 10.659953296948796, Validation Loss = 12.236786398567082\n",
      "Iteration 143: Training Loss = 10.644044018884763, Validation Loss = 12.213099306276906\n",
      "Iteration 144: Training Loss = 10.628870121081581, Validation Loss = 12.188419597608831\n",
      "Iteration 145: Training Loss = 10.613920501532682, Validation Loss = 12.162322482668346\n",
      "Iteration 146: Training Loss = 10.598929589671078, Validation Loss = 12.137251489771746\n",
      "Iteration 147: Training Loss = 10.584070873787082, Validation Loss = 12.111997805643401\n",
      "Iteration 148: Training Loss = 10.569681554503461, Validation Loss = 12.086541215167754\n",
      "Iteration 149: Training Loss = 10.55514777748152, Validation Loss = 12.06336331495932\n",
      "Iteration 150: Training Loss = 10.541302808670222, Validation Loss = 12.038323672729296\n",
      "Mean Squared Error on Validation Set: 12.038323672729296\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1          19.7124           24.42m\n",
      "         2          19.5169           24.54m\n",
      "         3          19.3255           23.89m\n",
      "         4          19.1374           24.91m\n",
      "         5          18.9530           24.82m\n",
      "         6          18.7721           24.58m\n",
      "         7          18.5934           24.14m\n",
      "         8          18.4190           23.89m\n",
      "         9          18.2469           23.53m\n",
      "        10          18.0796           23.30m\n",
      "        20          16.5532           21.39m\n",
      "        30          15.2736           19.56m\n",
      "        40          14.1930           17.83m\n",
      "        50          13.2794           16.28m\n",
      "        60          12.4974           14.70m\n",
      "        70          11.8340           13.02m\n",
      "        80          11.2660           11.40m\n",
      "        90          10.7826            9.77m\n",
      "       100          10.3689            8.14m\n",
      "Model for store WI_3 saved as model_store_WI_3_GB_0218.joblib\n",
      "Iteration 1: Training Loss = 19.712446844318908, Validation Loss = 14.344833710213656\n",
      "Iteration 2: Training Loss = 19.516936276036915, Validation Loss = 14.185056556046414\n",
      "Iteration 3: Training Loss = 19.32549978009392, Validation Loss = 14.027580062869657\n",
      "Iteration 4: Training Loss = 19.137375576704773, Validation Loss = 13.87370044543606\n",
      "Iteration 5: Training Loss = 18.952968639343883, Validation Loss = 13.720951483786065\n",
      "Iteration 6: Training Loss = 18.77214374754656, Validation Loss = 13.570447763718168\n",
      "Iteration 7: Training Loss = 18.593379708488275, Validation Loss = 13.423664875595039\n",
      "Iteration 8: Training Loss = 18.41900048954172, Validation Loss = 13.28066312380741\n",
      "Iteration 9: Training Loss = 18.246919628288317, Validation Loss = 13.139153722585437\n",
      "Iteration 10: Training Loss = 18.079562809995785, Validation Loss = 12.99937723009389\n",
      "Iteration 11: Training Loss = 17.914051730919013, Validation Loss = 12.864263980142555\n",
      "Iteration 12: Training Loss = 17.752414899728926, Validation Loss = 12.729946931266568\n",
      "Iteration 13: Training Loss = 17.592710977600216, Validation Loss = 12.593822838335276\n",
      "Iteration 14: Training Loss = 17.436324562084135, Validation Loss = 12.465931015125369\n",
      "Iteration 15: Training Loss = 17.28334557494731, Validation Loss = 12.341133591948688\n",
      "Iteration 16: Training Loss = 17.131605591438667, Validation Loss = 12.216780721335946\n",
      "Iteration 17: Training Loss = 16.98330012611721, Validation Loss = 12.092714788660846\n",
      "Iteration 18: Training Loss = 16.83702691681183, Validation Loss = 11.96957911147684\n",
      "Iteration 19: Training Loss = 16.693528793106555, Validation Loss = 11.851890480419156\n",
      "Iteration 20: Training Loss = 16.5531949703171, Validation Loss = 11.737370591372441\n",
      "Iteration 21: Training Loss = 16.41445729996442, Validation Loss = 11.623035841347797\n",
      "Iteration 22: Training Loss = 16.27864682464641, Validation Loss = 11.510997352632357\n",
      "Iteration 23: Training Loss = 16.145696111521595, Validation Loss = 11.400289851758977\n",
      "Iteration 24: Training Loss = 16.014056186311333, Validation Loss = 11.28780112146952\n",
      "Iteration 25: Training Loss = 15.885518611351113, Validation Loss = 11.180990677362232\n",
      "Iteration 26: Training Loss = 15.759631237621319, Validation Loss = 11.078007795876058\n",
      "Iteration 27: Training Loss = 15.634793048774576, Validation Loss = 10.975488887079873\n",
      "Iteration 28: Training Loss = 15.51294078530895, Validation Loss = 10.870232451167645\n",
      "Iteration 29: Training Loss = 15.39173783082129, Validation Loss = 10.768741567264003\n",
      "Iteration 30: Training Loss = 15.273634922662547, Validation Loss = 10.67115002084073\n",
      "Iteration 31: Training Loss = 15.15730585215108, Validation Loss = 10.571557246296292\n",
      "Iteration 32: Training Loss = 15.042403293414669, Validation Loss = 10.475062558397754\n",
      "Iteration 33: Training Loss = 14.929517718768158, Validation Loss = 10.377874642379764\n",
      "Iteration 34: Training Loss = 14.819528764285156, Validation Loss = 10.28659788171478\n",
      "Iteration 35: Training Loss = 14.709919345249745, Validation Loss = 10.195881441762873\n",
      "Iteration 36: Training Loss = 14.603142717197496, Validation Loss = 10.106007984209029\n",
      "Iteration 37: Training Loss = 14.498353767040115, Validation Loss = 10.018218502648862\n",
      "Iteration 38: Training Loss = 14.395320684847693, Validation Loss = 9.932233258307857\n",
      "Iteration 39: Training Loss = 14.293126150562495, Validation Loss = 9.850241106898006\n",
      "Iteration 40: Training Loss = 14.192978999674237, Validation Loss = 9.77020647274077\n",
      "Iteration 41: Training Loss = 14.095269709839977, Validation Loss = 9.686927701873563\n",
      "Iteration 42: Training Loss = 13.998433091715203, Validation Loss = 9.60620864180165\n",
      "Iteration 43: Training Loss = 13.903827870888449, Validation Loss = 9.526709016430019\n",
      "Iteration 44: Training Loss = 13.809954934043676, Validation Loss = 9.451851028169811\n",
      "Iteration 45: Training Loss = 13.718406131976193, Validation Loss = 9.379205840483724\n",
      "Iteration 46: Training Loss = 13.628389271040753, Validation Loss = 9.303460640066097\n",
      "Iteration 47: Training Loss = 13.536143678990394, Validation Loss = 9.233099900044987\n",
      "Iteration 48: Training Loss = 13.448727672581331, Validation Loss = 9.158963869264472\n",
      "Iteration 49: Training Loss = 13.363205255564939, Validation Loss = 9.08875928121607\n",
      "Iteration 50: Training Loss = 13.279379272025198, Validation Loss = 9.018278751584868\n",
      "Iteration 51: Training Loss = 13.193069536494285, Validation Loss = 8.952692721878366\n",
      "Iteration 52: Training Loss = 13.112087504298849, Validation Loss = 8.881601075635277\n",
      "Iteration 53: Training Loss = 13.02859316009898, Validation Loss = 8.81820512526043\n",
      "Iteration 54: Training Loss = 12.949505574198211, Validation Loss = 8.752036802771944\n",
      "Iteration 55: Training Loss = 12.87194067454984, Validation Loss = 8.690940146781596\n",
      "Iteration 56: Training Loss = 12.795554492272418, Validation Loss = 8.627462213080705\n",
      "Iteration 57: Training Loss = 12.717383574919243, Validation Loss = 8.56800046820727\n",
      "Iteration 58: Training Loss = 12.643859275701491, Validation Loss = 8.506089908584919\n",
      "Iteration 59: Training Loss = 12.568721701415864, Validation Loss = 8.449287580995092\n",
      "Iteration 60: Training Loss = 12.497400267490152, Validation Loss = 8.390202300432223\n",
      "Iteration 61: Training Loss = 12.42689866384212, Validation Loss = 8.338430718071281\n",
      "Iteration 62: Training Loss = 12.356601922162383, Validation Loss = 8.28335304292496\n",
      "Iteration 63: Training Loss = 12.285742253900413, Validation Loss = 8.229794702776832\n",
      "Iteration 64: Training Loss = 12.21899060762918, Validation Loss = 8.177765347848826\n",
      "Iteration 65: Training Loss = 12.153081947388543, Validation Loss = 8.12135467039118\n",
      "Iteration 66: Training Loss = 12.086083664567639, Validation Loss = 8.069334661058592\n",
      "Iteration 67: Training Loss = 12.022006346862273, Validation Loss = 8.019770488115858\n",
      "Iteration 68: Training Loss = 11.956646171372189, Validation Loss = 7.970790688822449\n",
      "Iteration 69: Training Loss = 11.894973427704585, Validation Loss = 7.922746612261705\n",
      "Iteration 70: Training Loss = 11.833998435782423, Validation Loss = 7.874411352311954\n",
      "Iteration 71: Training Loss = 11.77150978412803, Validation Loss = 7.82476196126574\n",
      "Iteration 72: Training Loss = 11.71250702769846, Validation Loss = 7.778168663829083\n",
      "Iteration 73: Training Loss = 11.654487411150384, Validation Loss = 7.7306410158446965\n",
      "Iteration 74: Training Loss = 11.595448221584228, Validation Loss = 7.684418771433128\n",
      "Iteration 75: Training Loss = 11.538444307237341, Validation Loss = 7.639926972613908\n",
      "Iteration 76: Training Loss = 11.483571738244809, Validation Loss = 7.594451966114011\n",
      "Iteration 77: Training Loss = 11.426718153140342, Validation Loss = 7.5526848626328595\n",
      "Iteration 78: Training Loss = 11.372980561528449, Validation Loss = 7.51038971302185\n",
      "Iteration 79: Training Loss = 11.320474519041882, Validation Loss = 7.468340136863957\n",
      "Iteration 80: Training Loss = 11.266017498579304, Validation Loss = 7.428091851185773\n",
      "Iteration 81: Training Loss = 11.215476713122495, Validation Loss = 7.38742579974696\n",
      "Iteration 82: Training Loss = 11.1653590072868, Validation Loss = 7.34768250008207\n",
      "Iteration 83: Training Loss = 11.114295246076301, Validation Loss = 7.30790363004277\n",
      "Iteration 84: Training Loss = 11.065086048001186, Validation Loss = 7.268851582583193\n",
      "Iteration 85: Training Loss = 11.016906794349483, Validation Loss = 7.231295112001041\n",
      "Iteration 86: Training Loss = 10.967326634122422, Validation Loss = 7.19481222984633\n",
      "Iteration 87: Training Loss = 10.919955731445073, Validation Loss = 7.160222692563022\n",
      "Iteration 88: Training Loss = 10.873689653773576, Validation Loss = 7.1235290439778485\n",
      "Iteration 89: Training Loss = 10.829012119467968, Validation Loss = 7.084710076727603\n",
      "Iteration 90: Training Loss = 10.78257874382121, Validation Loss = 7.050707476678829\n",
      "Iteration 91: Training Loss = 10.738597561618649, Validation Loss = 7.016239905341901\n",
      "Iteration 92: Training Loss = 10.695898322018097, Validation Loss = 6.980601257447574\n",
      "Iteration 93: Training Loss = 10.651266167786638, Validation Loss = 6.948018591844078\n",
      "Iteration 94: Training Loss = 10.609141881129515, Validation Loss = 6.916857929563198\n",
      "Iteration 95: Training Loss = 10.567891916329017, Validation Loss = 6.883024303486559\n",
      "Iteration 96: Training Loss = 10.525572260222535, Validation Loss = 6.85200173204032\n",
      "Iteration 97: Training Loss = 10.485797171415465, Validation Loss = 6.820680069999407\n",
      "Iteration 98: Training Loss = 10.446410351316128, Validation Loss = 6.789474652418796\n",
      "Iteration 99: Training Loss = 10.40806153407749, Validation Loss = 6.758890359683553\n",
      "Iteration 100: Training Loss = 10.368899508973726, Validation Loss = 6.730255779597293\n",
      "Iteration 101: Training Loss = 10.331764686758738, Validation Loss = 6.698744781747092\n",
      "Iteration 102: Training Loss = 10.29240965612235, Validation Loss = 6.6693570483170594\n",
      "Iteration 103: Training Loss = 10.256278659057488, Validation Loss = 6.639094934537815\n",
      "Iteration 104: Training Loss = 10.218260802857051, Validation Loss = 6.6105489372999005\n",
      "Iteration 105: Training Loss = 10.183058014935769, Validation Loss = 6.581511083455884\n",
      "Iteration 106: Training Loss = 10.14663076807691, Validation Loss = 6.555608503244133\n",
      "Iteration 107: Training Loss = 10.111844477243514, Validation Loss = 6.528644522240487\n",
      "Iteration 108: Training Loss = 10.078237043291292, Validation Loss = 6.501634194711557\n",
      "Iteration 109: Training Loss = 10.042664965646264, Validation Loss = 6.474938900394802\n",
      "Iteration 110: Training Loss = 10.01028244771563, Validation Loss = 6.449222082120399\n",
      "Iteration 111: Training Loss = 9.978246346043928, Validation Loss = 6.422916948801608\n",
      "Iteration 112: Training Loss = 9.945451538405628, Validation Loss = 6.398604117471116\n",
      "Iteration 113: Training Loss = 9.914720277792389, Validation Loss = 6.37289491802167\n",
      "Iteration 114: Training Loss = 9.884724476568236, Validation Loss = 6.347027771397401\n",
      "Iteration 115: Training Loss = 9.853257337435116, Validation Loss = 6.324301418550633\n",
      "Iteration 116: Training Loss = 9.82391268062393, Validation Loss = 6.300070601944213\n",
      "Iteration 117: Training Loss = 9.794817298742249, Validation Loss = 6.275867804651416\n",
      "Iteration 118: Training Loss = 9.763848805319158, Validation Loss = 6.252540714495809\n",
      "Iteration 119: Training Loss = 9.734092910886867, Validation Loss = 6.23153422350162\n",
      "Iteration 120: Training Loss = 9.706428191575105, Validation Loss = 6.2090604457492855\n",
      "Iteration 121: Training Loss = 9.67897496116164, Validation Loss = 6.186228467213406\n",
      "Iteration 122: Training Loss = 9.649880178148198, Validation Loss = 6.164666594044536\n",
      "Iteration 123: Training Loss = 9.623543422494341, Validation Loss = 6.1440392210885175\n",
      "Iteration 124: Training Loss = 9.59637099203206, Validation Loss = 6.1243634719798\n",
      "Iteration 125: Training Loss = 9.570648322255549, Validation Loss = 6.103990443869896\n",
      "Iteration 126: Training Loss = 9.544220173508908, Validation Loss = 6.084670425904878\n",
      "Iteration 127: Training Loss = 9.518933619410388, Validation Loss = 6.063376564473792\n",
      "Iteration 128: Training Loss = 9.494273322969677, Validation Loss = 6.044075439087894\n",
      "Iteration 129: Training Loss = 9.470167609765767, Validation Loss = 6.024441251001732\n",
      "Iteration 130: Training Loss = 9.444457223254064, Validation Loss = 6.005306422491979\n",
      "Iteration 131: Training Loss = 9.42140850178583, Validation Loss = 5.985857469447544\n",
      "Iteration 132: Training Loss = 9.396965998375654, Validation Loss = 5.96751372210655\n",
      "Iteration 133: Training Loss = 9.37453603035989, Validation Loss = 5.949610908849887\n",
      "Iteration 134: Training Loss = 9.350940511711032, Validation Loss = 5.933029065228422\n",
      "Iteration 135: Training Loss = 9.328682075871386, Validation Loss = 5.91549324050738\n",
      "Iteration 136: Training Loss = 9.307075577357672, Validation Loss = 5.89680430224791\n",
      "Iteration 137: Training Loss = 9.283960579746616, Validation Loss = 5.879711072982311\n",
      "Iteration 138: Training Loss = 9.261705898903497, Validation Loss = 5.862990944114046\n",
      "Iteration 139: Training Loss = 9.241169326795646, Validation Loss = 5.8458293019992835\n",
      "Iteration 140: Training Loss = 9.22067267410922, Validation Loss = 5.829885205585242\n",
      "Iteration 141: Training Loss = 9.199376268671521, Validation Loss = 5.81398788153797\n",
      "Iteration 142: Training Loss = 9.179883905925895, Validation Loss = 5.798543617811961\n",
      "Iteration 143: Training Loss = 9.16049414975328, Validation Loss = 5.782194334650421\n",
      "Iteration 144: Training Loss = 9.139728197453103, Validation Loss = 5.766826433045545\n",
      "Iteration 145: Training Loss = 9.120781168818008, Validation Loss = 5.7522952684936834\n",
      "Iteration 146: Training Loss = 9.101047991254205, Validation Loss = 5.737488153871022\n",
      "Iteration 147: Training Loss = 9.082811506606879, Validation Loss = 5.723248462712808\n",
      "Iteration 148: Training Loss = 9.063659584132989, Validation Loss = 5.70957170927346\n",
      "Iteration 149: Training Loss = 9.046193646058136, Validation Loss = 5.695839968942662\n",
      "Iteration 150: Training Loss = 9.027691353060938, Validation Loss = 5.682350046480671\n",
      "Mean Squared Error on Validation Set: 5.682350046480671\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "n_estimators = 150  # Number of boosting stages (iterations)\n",
    "learning_rate = 0.01  # Shrinkage rate\n",
    "\n",
    "for i in range(len(STORES_IDS)):\n",
    "    df, features = get_data_by_store(STORES_IDS[i])\n",
    "    features.append('sales')\n",
    "    features.append('d')\n",
    "    columns_to_remove = ['id']\n",
    "    features = list(df.columns)\n",
    "    for col in columns_to_remove:\n",
    "        features.remove(col)\n",
    "    \n",
    "    df = df[features]\n",
    "    df.dropna(inplace=True)\n",
    "    train_df, validation_df = split(df)\n",
    "    features.remove('d')\n",
    "    train_df = train_df[features]\n",
    "    validation_df = validation_df[features]\n",
    "\n",
    "    X_train = train_df.drop(columns=['sales'])\n",
    "    y_train = train_df['sales']\n",
    "\n",
    "    X_val = validation_df.drop(columns=['sales'])\n",
    "    y_val = validation_df['sales']\n",
    "\n",
    "    gb_model = GradientBoostingRegressor(loss='squared_error', \n",
    "                                         n_estimators=n_estimators, \n",
    "                                         learning_rate=learning_rate,\n",
    "                                         random_state=66, \n",
    "                                         verbose=1)\n",
    "\n",
    "    gb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Compute loss values for each boosting stage on the training set\n",
    "    train_loss = [mean_squared_error(y_train, y_pred) for y_pred in gb_model.staged_predict(X_train)]\n",
    "    \n",
    "    # Compute loss values for each boosting stage on the validation set\n",
    "    val_loss = [mean_squared_error(y_val, y_pred) for y_pred in gb_model.staged_predict(X_val)]\n",
    "\n",
    "    # Save the trained model to a file\n",
    "    model_filename = f'model_store_{STORES_IDS[i]}_GB_0218.joblib'\n",
    "    joblib.dump(gb_model, model_filename)\n",
    "    print(f\"Model for store {STORES_IDS[i]} saved as {model_filename}\")\n",
    "\n",
    "    # Print loss values for each boosting stage\n",
    "    for stage, (train_loss_value, val_loss_value) in enumerate(zip(train_loss, val_loss), start=1):\n",
    "        print(f\"Iteration {stage}: Training Loss = {train_loss_value}, Validation Loss = {val_loss_value}\")\n",
    "\n",
    "    y_val_pred = gb_model.predict(X_val)\n",
    "\n",
    "    mse = mean_squared_error(y_val, y_val_pred)\n",
    "    print(\"Mean Squared Error on Validation Set:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "predicted_df = pd.DataFrame(columns=['id', 'prediction'])\n",
    "\n",
    "for i in range(len(STORES_IDS)):\n",
    "    # Load the saved model\n",
    "    loaded_model = joblib.load(f'model_store_{STORES_IDS[i]}_GB.joblib')\n",
    "\n",
    "    df, _ = get_data_by_store(STORES_IDS[i])\n",
    "    df = pd.get_dummies(df, columns=['event_name_1'])\n",
    "    columns_to_remove = ['id']\n",
    "    features = list(df.columns)\n",
    "    for col in columns_to_remove:\n",
    "        features.remove(col)\n",
    "    test = df[df[\"d\"] > END_TRAIN]\n",
    "    id = test[\"id\"]\n",
    "    features.remove('d')\n",
    "    X_test = test[features]\n",
    "    X_test = X_test.drop(columns=['sales'])\n",
    "    \n",
    "    imputer = SimpleImputer(strategy='median')  # You can choose 'mean', 'median', 'most_frequent', or a constant value\n",
    "    X_test = imputer.fit_transform(X_test)\n",
    "    # Use the loaded model for prediction\n",
    "    prediction = loaded_model.predict(X_test)\n",
    "\n",
    "    print(f\"--Predition of {STORES_IDS[i]}--\")\n",
    "    print(id)\n",
    "    print(prediction)\n",
    "\n",
    "    temp_df = pd.DataFrame({'id': id, 'prediction': prediction})\n",
    "    temp_df.to_csv(f\"results_GB_{STORES_IDS[i]}_0218.csv\",index=False)\n",
    "    print(f\"Predicted Time Ahead: {len(temp_df)}\")\n",
    "    # Concatenate the temporary DataFrame to the main DataFrame along the row direction\n",
    "    predicted_df = pd.concat([predicted_df, temp_df])\n",
    "    predicted_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "\n",
    "raw_data_dir = 'datasets\\sales_train_evaluation.csv'\n",
    "processed_data_dir = 'datasets\\\\'\n",
    "ORIGINAL = raw_data_dir\n",
    "BASE     = processed_data_dir+'grid_part_1.pkl'\n",
    "PRICE    = processed_data_dir+'grid_part_2.pkl'\n",
    "CALENDAR = processed_data_dir+'grid_part_3.pkl'\n",
    "LAGS     = processed_data_dir+'lags_df_28.pkl'\n",
    "MEAN_ENC = processed_data_dir+'mean_encoding_df.pkl'\n",
    "\n",
    "STORES_IDS = ['CA_1','CA_2','CA_3','CA_4','TX_1','TX_2','TX_3','WI_1','WI_2','WI_3']\n",
    "\n",
    "#LIMITS and const\n",
    "TARGET      = 'sales'            \n",
    "START_TRAIN = 0                  \n",
    "END_TRAIN   = 1941\n",
    "P_HORIZON   = 28\n",
    "\n",
    "mean_features   = ['enc_cat_id_mean','enc_cat_id_std',\n",
    "                   'enc_dept_id_mean','enc_dept_id_std',\n",
    "                   'enc_item_id_mean','enc_item_id_std'] \n",
    "\n",
    "remove_features = [TARGET,'id','state_id','store_id',\n",
    "                    'item_id', 'dept_id', 'cat_id','date','wm_yr_wk','d', \\\n",
    "                    'release', 'sell_price', 'price_max', 'price_min', \\\n",
    "                    'price_norm', 'price_nunique', 'item_nunique', \\\n",
    "                    'price_momentum', 'price_momentum_m', 'price_momentum_y', \\\n",
    "                    'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', \\\n",
    "                    'enc_cat_id_std', 'event_type_1', 'event_name_2', 'event_type_2', \\\n",
    "                    'enc_dept_id_std', 'enc_item_id_std', \\\n",
    "                    'rolling_std_7', 'rolling_std_14', \\\n",
    "                    'rolling_std_30', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', \\\n",
    "                    'rolling_mean_tmp_1_30', 'rolling_mean_tmp_7_7', \\\n",
    "                    'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30', \\\n",
    "                    'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', \\\n",
    "                    'rolling_mean_tmp_14_30']\n",
    "\n",
    "# Read data\n",
    "def get_data_by_store(store):\n",
    "    \n",
    "    # Read and contact basic feature\n",
    "    df = pd.concat([pd.read_pickle(BASE),\n",
    "                    pd.read_pickle(PRICE).iloc[:,2:],\n",
    "                    pd.read_pickle(CALENDAR).iloc[:,2:]],\n",
    "                    axis=1)\n",
    "    \n",
    "\n",
    "    df = df[df['d']>=START_TRAIN]\n",
    "    \n",
    "    df = df[df['store_id']==store]\n",
    "\n",
    "    df2 = pd.read_pickle(MEAN_ENC)[mean_features]\n",
    "    df2 = df2[df2.index.isin(df.index)]\n",
    "    \n",
    "    df3 = pd.read_pickle(LAGS).iloc[:,3:]\n",
    "    df3 = df3[df3.index.isin(df.index)]\n",
    "    \n",
    "    df = pd.concat([df, df2], axis=1)\n",
    "    del df2\n",
    "    \n",
    "    df = pd.concat([df, df3], axis=1)\n",
    "    del df3\n",
    "\n",
    "    state = \"snap_\" + store.split('_')[0]\n",
    "    states = ['snap_CA','snap_TX','snap_WI']\n",
    "    deleted_states = []\n",
    "    for i in states:\n",
    "        if i != state:\n",
    "            deleted_states.append(i)\n",
    "            \n",
    "    features = [col for col in list(df) if (col not in remove_features and col not in deleted_states)]\n",
    "    \n",
    "    df = df[['id','d',TARGET]+features]\n",
    "    \n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df, features\n",
    "\n",
    "# Recombine Test set after training\n",
    "def get_base_test():\n",
    "    base_test = pd.DataFrame()\n",
    "\n",
    "    for store_id in STORES_IDS:\n",
    "        temp_df = pd.read_pickle(processed_data_dir+'test_'+store_id+'.pkl')\n",
    "        temp_df['store_id'] = store_id\n",
    "        base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)\n",
    "    \n",
    "    return base_test\n",
    "\n",
    "# split(trainning and validation)\n",
    "def split(dataframe_for_splitting):\n",
    "    train = dataframe_for_splitting[dataframe_for_splitting[\"d\"] <= END_TRAIN - P_HORIZON]\n",
    "    validation = dataframe_for_splitting[(dataframe_for_splitting[\"d\"] <= END_TRAIN) & (dataframe_for_splitting[\"d\"] > END_TRAIN - P_HORIZON)]\n",
    "    return train, validation\n",
    "\n",
    "\n",
    "import joblib\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "for i in range(len(STORES_IDS)):\n",
    "    # Load the saved model\n",
    "    loaded_model = joblib.load(f'model_store_{STORES_IDS[i]}_GB.joblib')\n",
    "\n",
    "    df, _ = get_data_by_store(STORES_IDS[i])\n",
    "    df = pd.get_dummies(df, columns=['event_name_1'])\n",
    "    columns_to_remove = ['id']\n",
    "    features = list(df.columns)\n",
    "    for col in columns_to_remove:\n",
    "        features.remove(col)\n",
    "    test = df[df[\"d\"] > END_TRAIN]\n",
    "    id = test[\"id\"]\n",
    "    d = test[\"d\"]\n",
    "    features.remove('d')\n",
    "    X_test = test[features]\n",
    "    X_test = X_test.drop(columns=['sales'])\n",
    "    \n",
    "    imputer = SimpleImputer(strategy='median')  # You can choose 'mean', 'median', 'most_frequent', or a constant value\n",
    "    X_test = imputer.fit_transform(X_test)\n",
    "    # Use the loaded model for prediction\n",
    "    prediction = loaded_model.predict(X_test)\n",
    "\n",
    "    print(f\"--Predition of {STORES_IDS[i]}--\")\n",
    "    print(id)\n",
    "    print(prediction)\n",
    "\n",
    "    temp_df = pd.DataFrame({'id': id,'d': d, 'prediction': prediction})\n",
    "    temp_df.to_csv(f\"results_GB_{STORES_IDS[i]}.csv\",index=False)\n",
    "    print(f\"Predicted Time Ahead: {len(temp_df)}\")\n",
    "    \n",
    "    \n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(\"results_GB_CA_1.csv\")\n",
    "\n",
    "for i in range(1,len(STORES_IDS)):    \n",
    "    df2 = pd.read_csv(f\"results_GB_{STORES_IDS[i]}.csv\")\n",
    "\n",
    "# Concatenate the two DataFrames vertically (along the rows)\n",
    "    df1 = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Save the concatenated DataFrame to a new CSV file\n",
    "df1.to_csv(\"concatenated_file.csv\", index=False)\n",
    "\n",
    "df1 = pd.read_csv(\"concatenated_file.csv\")\n",
    "\n",
    "pivot_table = df1.pivot_table(index='id', columns='d', values='prediction', aggfunc='median')\n",
    "\n",
    "pivot_table.to_csv(\"pivot_file.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Save the concatenated DataFrame to a new CSV file\n",
    "new_column_names = []\n",
    "for i in range(28):\n",
    "    new_column_names.append(f\"F{i+1}\")\n",
    "\n",
    "df1 = pd.read_csv(\"concatenated_file.csv\")\n",
    "\n",
    "pivot_table = df1.pivot_table(index='id', columns='d', values='prediction', aggfunc='median')\n",
    "pivot_table.columns = new_column_names\n",
    "\n",
    "pivot_table.to_csv(\"pivot_file.csv\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the first CSV file\n",
    "df1 = pd.read_csv(\"datasets\\sample_submission.csv\")\n",
    "\n",
    "# Read the second CSV file\n",
    "df2 = pd.read_csv(\"pivot_file.csv\")\n",
    "\n",
    "# Merge the two DataFrames on the 'id' column\n",
    "merged_df = pd.merge(df1, df2, on='id', how=\"left\", suffixes=('_original', ''))\n",
    "\n",
    "# Replace values in columns F1 to F28 in df1 with corresponding values from df2\n",
    "for col in df1.columns[1:]:\n",
    "    merged_df[col+'_original'] = merged_df[col+'_original'].fillna(merged_df[col])\n",
    "\n",
    "# Drop the extra columns\n",
    "merged_df.drop(columns=[col+'_original' for col in df1.columns[1:]], inplace=True)\n",
    "merged_df.fillna(0, inplace=True)\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "merged_df.to_csv(\"merged_file.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
